{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np;\n",
    "import time\n",
    "\n",
    "\n",
    "# create Theano variables for input and target minibatch\n",
    "sentence1 = T.ftensor3('s1')\n",
    "sentence2 = T.ftensor3('s2')\n",
    "mask1 = T.bmatrix('m1')\n",
    "mask2 = T.bmatrix('m2')\n",
    "target_var = T.ivector('ent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a small convolutional neural network\n",
    "from lasagne.nonlinearities import leaky_rectify, softmax\n",
    "from lasagne.layers import InputLayer, Conv2DLayer, Pool2DLayer, LSTMLayer, ConcatLayer, DenseLayer, dropout, Conv1DLayer, ReshapeLayer\n",
    "\n",
    "from const import *\n",
    "\n",
    "\n",
    "import dataIO\n",
    "\n",
    "def createNeuralNetwork():\n",
    "    #(batchsize, sequence length, onehot vector length)\n",
    "    in1 = InputLayer((None, None, kNUM_CHARS), sentence1)\n",
    "    in2 = InputLayer((None, None, kNUM_CHARS), sentence2)\n",
    "    l_mask1=lasagne.layers.InputLayer((None,None), mask1)\n",
    "    l_mask2=lasagne.layers.InputLayer((None,None), mask2)\n",
    "\n",
    "    num_LSTM_output = (512)\n",
    "    lstm1_f = LSTMLayer(in1, num_LSTM_output,\n",
    "        forgetgate=lasagne.layers.Gate(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        cell_init=lasagne.init.Constant(0.),\n",
    "        hid_init=lasagne.init.Constant(0.), grad_clipping=kGRAD_CLIP,\n",
    "        backwards=False,\n",
    "        mask_input=l_mask1,\n",
    "        only_return_final=True)\n",
    "    lstm1_b = LSTMLayer(in1, num_LSTM_output,\n",
    "        forgetgate=lasagne.layers.Gate(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        cell_init=lasagne.init.Constant(0.),\n",
    "        hid_init=lasagne.init.Constant(0.), grad_clipping=kGRAD_CLIP,\n",
    "        backwards=True,\n",
    "        mask_input=l_mask1,\n",
    "        only_return_final=True)\n",
    "\n",
    "    lstm2_f = LSTMLayer(in2, num_LSTM_output,\n",
    "        forgetgate=lasagne.layers.Gate(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        cell_init=lasagne.init.Constant(0.),\n",
    "        hid_init=lasagne.init.Constant(0.), grad_clipping=kGRAD_CLIP,\n",
    "        backwards=False,\n",
    "        mask_input=l_mask2,\n",
    "        only_return_final=True)\n",
    "    lstm2_b = LSTMLayer(in2, num_LSTM_output,\n",
    "        forgetgate=lasagne.layers.Gate(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        cell_init=lasagne.init.Constant(0.),\n",
    "        hid_init=lasagne.init.Constant(0.), grad_clipping=kGRAD_CLIP,\n",
    "        backwards=True,\n",
    "        mask_input=l_mask2,\n",
    "        only_return_final=True)\n",
    "\n",
    "    network = ConcatLayer([lstm1_f, lstm1_b, lstm2_f, lstm2_b], axis=1) #(NONE-sentencesize by 2048)\n",
    "    network = ReshapeLayer(network, (-1, 1, 4, num_LSTM_output));\n",
    "    #(None by 4x512) I think.\n",
    "    network = Conv2DLayer(network, 20, (3,3), pad='same',\n",
    "                                         nonlinearity=leaky_rectify)\n",
    "    #(20 by 4 by 52)\n",
    "    network = Conv2DLayer(network, 10, (3,3), pad='same',\n",
    "                                         nonlinearity=leaky_rectify)\n",
    "\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, (4,4), stride=2)\n",
    "\n",
    "    network = DenseLayer(dropout(network, 0.5),\n",
    "                                        128, nonlinearity=leaky_rectify,\n",
    "                                        W=lasagne.init.Orthogonal())\n",
    "\n",
    "    network = DenseLayer(dropout(network, 0.5), 3, nonlinearity=softmax)\n",
    "\n",
    "    return network;\n",
    "\n",
    "\n",
    "network = createNeuralNetwork();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOLY SHIT IT COMPILED\n",
      "HOLY SHIT IT COMPILED OUTPUT\n",
      "HOLY SHIT IT COMPILED LOSS FUNCTION WHAT\n",
      "HOLY SHIT IT COMPILED UPDATES\n",
      "HOLY SHIT IT COMPILED TRAINING FUNCTION\n"
     ]
    }
   ],
   "source": [
    "def loadDriverModelFromFile(filename):\n",
    "    print \"Loading Neural Network Values from File\"\n",
    "    _v = np.load(filename)['model']\n",
    "    lasagne.layers.set_all_param_values(network, _v)\n",
    "    if type(filename)==str:\n",
    "        print \"LOADED\"\n",
    "        return;\n",
    "    print \"Loaded!\"\n",
    "\n",
    "print \"HOLY SHIT IT COMPILED\"\n",
    "# create loss function\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "print \"HOLY SHIT IT COMPILED OUTPUT\"\n",
    "\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean() + 1e-4 * lasagne.regularization.regularize_network_params(\n",
    "        network, lasagne.regularization.l2)\n",
    "print \"HOLY SHIT IT COMPILED LOSS FUNCTION WHAT\"\n",
    "\n",
    "# create parameter update expressions\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01,\n",
    "                                            momentum=0.9)\n",
    "print \"HOLY SHIT IT COMPILED UPDATES\"\n",
    "\n",
    "# compile training function that updates parameters and returns training loss\n",
    "train_fn = theano.function([sentence1, sentence2, mask1, mask2, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "# train_fn = theano.function([sentence1, sentence2, target_var], loss, updates=updates)\n",
    "print \"HOLY SHIT IT COMPILED TRAINING FUNCTION\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOLY SHIT IT EPOCHS!\n",
      "yeahhh\n",
      "yeahhh\n",
      "yeahhh\n",
      "yeahhh\n",
      "yeahhh\n",
      "yeahhh\n",
      "yeahhh\n",
      "yeahhh\n",
      "yeahhh\n",
      "yeahhh\n",
      "yeahhh\n",
      "yeahhh\n"
     ]
    }
   ],
   "source": [
    "# train network (assuming you've got some training data in numpy arrays)\n",
    "\n",
    "def makeMask(batch): #list of matricies of variable size.\n",
    "    m = np.zeros((len(batch), max([b.shape[0] for b in batch])))\n",
    "    for i,b in enumerate(batch):\n",
    "        m[i, 0:b.shape[0]] = 1.0\n",
    "    return m;\n",
    "\n",
    "for epoch in range(kNUM_EPOCHS):\n",
    "    batchCount = 0\n",
    "    loss = 0\n",
    "    print \"HOLY SHIT IT EPOCHS!\"\n",
    "    #for (batch1,m1), (batch2,m2), ys in dataIO.readChunk(kBATCH_SIZE, './snli_1.0/snli_1.0_train.txt'):\n",
    "    for (batch1,m1), (batch2,m2), ys in dataIO.readChunk(kBATCH_SIZE, './snli_1.0/snli_1.0_train.txt'):\n",
    "        if batchCount >= kMAX_BATCHES:\n",
    "            break\n",
    "        # import code\n",
    "        # code.interact(local=locals())\n",
    "        # print batch1.shape, batch2.shape, ys.shape\n",
    "        # loss += train_fn(batch1, batch2, makeMask(batch1), makeMask(batch2), ys)\n",
    "        loss += train_fn(batch1, batch2, m1, m2, ys)\n",
    "        batchCount += 1\n",
    "        print \"yeahhh\"\n",
    "\n",
    "    print(\"Epoch %d: Loss %g\" % (epoch + 1, loss / batchCount))\n",
    "   \n",
    "    vals = lasagne.layers.get_all_param_values(network)\n",
    "    modelFile = open(\"modelStore/\"+time.strftime(\"%m%d-%H%M%S\")+\".pkl\", mode=\"w\")    \n",
    "    np.savez(modelFile, model=vals)\n",
    "    print \">>>\", modelFile.name\n",
    "    modelFile.close()\n",
    "    print \"Done.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save model\n",
    "\n",
    "# use trained network for predictions\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "predict_fn = theano.function([sentence1, sentence2, mask1, mask2], T.argmax(test_prediction, axis=1))\n",
    "total_classifications = 0\n",
    "total_correct = 0\n",
    "total_incorrect = 0\n",
    "for (batch1,m1), (batch2,m2), ys in dataIO.readChunk(kBATCH_SIZE, './snli_1.0/snli_1.0_dev.txt'):\n",
    "    if predict_fn(batch1, batch2, m1, m2) == ys:\n",
    "        total_correct += 1\n",
    "    else:\n",
    "        total_incorrect += 1\n",
    "    total_classifications += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stats print out\n",
    "\n",
    "print \"Total classifications: \" + str(total_classifications)\n",
    "print \"Total correct: \" + str(total_correct)\n",
    "print \"Total incorrect: \" + str(total_incorrect)\n",
    "print \"Accuracy: \" + str(float(total_correct/classifications))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
