{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 1: GeForce GTX 780 (CNMeM is enabled with initial size: 90.0% of memory, cuDNN 4007)\n",
      "/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np;\n",
    "import time\n",
    "\n",
    "\n",
    "# create Theano variables for input and target minibatch\n",
    "sentence1 = T.ftensor3('s1')\n",
    "sentence2 = T.ftensor3('s2')\n",
    "mask1 = T.bmatrix('m1')\n",
    "mask2 = T.bmatrix('m2')\n",
    "target_var = T.ivector('ent')\n",
    "MAX_BATCHES = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a small convolutional neural network\n",
    "from lasagne.nonlinearities import leaky_rectify, softmax\n",
    "from lasagne.layers import InputLayer, Conv2DLayer, Pool2DLayer, LSTMLayer, ConcatLayer, DenseLayer, dropout, Conv1DLayer, ReshapeLayer\n",
    "\n",
    "from const import *\n",
    "\n",
    "\n",
    "def createNeuralNetwork():\n",
    "    #(batchsize, sequence length, onehot vector length)\n",
    "    in1 = InputLayer((None, None, kNUM_CHARS), sentence1)\n",
    "    in2 = InputLayer((None, None, kNUM_CHARS), sentence2)\n",
    "    l_mask1=lasagne.layers.InputLayer((None,None), mask1)\n",
    "    l_mask2=lasagne.layers.InputLayer((None,None), mask2)\n",
    "\n",
    "    num_LSTM_output = (512)\n",
    "    lstm1_f = LSTMLayer(in1, num_LSTM_output,\n",
    "        forgetgate=lasagne.layers.Gate(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        cell_init=lasagne.init.Constant(0.),\n",
    "        hid_init=lasagne.init.Constant(0.), grad_clipping=kGRAD_CLIP,\n",
    "        backwards=False,\n",
    "        mask_input=l_mask1,\n",
    "        only_return_final=True)\n",
    "    lstm1_b = LSTMLayer(in1, num_LSTM_output,\n",
    "        forgetgate=lasagne.layers.Gate(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        cell_init=lasagne.init.Constant(0.),\n",
    "        hid_init=lasagne.init.Constant(0.), grad_clipping=kGRAD_CLIP,\n",
    "        backwards=True,\n",
    "        mask_input=l_mask1,\n",
    "        only_return_final=True)\n",
    "\n",
    "    lstm2_f = LSTMLayer(in2, num_LSTM_output,\n",
    "        forgetgate=lasagne.layers.Gate(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        cell_init=lasagne.init.Constant(0.),\n",
    "        hid_init=lasagne.init.Constant(0.), grad_clipping=kGRAD_CLIP,\n",
    "        backwards=False,\n",
    "        mask_input=l_mask2,\n",
    "        only_return_final=True)\n",
    "    lstm2_b = LSTMLayer(in2, num_LSTM_output,\n",
    "        forgetgate=lasagne.layers.Gate(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        cell_init=lasagne.init.Constant(0.),\n",
    "        hid_init=lasagne.init.Constant(0.), grad_clipping=kGRAD_CLIP,\n",
    "        backwards=True,\n",
    "        mask_input=l_mask2,\n",
    "        only_return_final=True)\n",
    "\n",
    "    network = ConcatLayer([lstm1_f, lstm1_b, lstm2_f, lstm2_b], axis=1) #(NONE-sentencesize by 2048)\n",
    "    network = ReshapeLayer(network, (-1, 1, 4, num_LSTM_output));\n",
    "    #(None by 4x512) I think.\n",
    "    network = Conv2DLayer(network, 20, (3,3), pad='same',\n",
    "                                         nonlinearity=leaky_rectify)\n",
    "    #(20 by 4 by 52)\n",
    "    network = Conv2DLayer(network, 10, (3,3), pad='same',\n",
    "                                         nonlinearity=leaky_rectify)\n",
    "\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, (4,4), stride=2)\n",
    "\n",
    "    network = DenseLayer(dropout(network, 0.5),\n",
    "                                        128, nonlinearity=leaky_rectify,\n",
    "                                        W=lasagne.init.Orthogonal())\n",
    "\n",
    "    network = DenseLayer(dropout(network, 0.5), 3, nonlinearity=softmax)\n",
    "\n",
    "    return network;\n",
    "\n",
    "\n",
    "network = createNeuralNetwork();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Neural Network Values from File\n",
      "Loaded!\n",
      "HOLY SHIT IT COMPILED\n",
      "HOLY SHIT IT COMPILED OUTPUT\n",
      "HOLY SHIT IT COMPILED LOSS FUNCTION WHAT\n",
      "HOLY SHIT IT COMPILED UPDATES\n",
      "HOLY SHIT IT COMPILED TRAINING FUNCTION\n"
     ]
    }
   ],
   "source": [
    "def loadDriverModelFromFile(filename):\n",
    "    print \"Loading Neural Network Values from File\"\n",
    "    _v = np.load(filename)['model']\n",
    "    lasagne.layers.set_all_param_values(network, _v)\n",
    "    if type(filename)==str:\n",
    "        print \"LOADED\"\n",
    "        return;\n",
    "    print \"Loaded!\"\n",
    "\n",
    "with open(\"./modelStore/0602-165522.pkl\", 'r') as f:\n",
    "    loadDriverModelFromFile(f)\n",
    "\n",
    "print \"HOLY SHIT IT COMPILED\"\n",
    "# create loss function\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "print \"HOLY SHIT IT COMPILED OUTPUT\"\n",
    "\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean() + 1e-4 * lasagne.regularization.regularize_network_params(\n",
    "        network, lasagne.regularization.l2)\n",
    "print \"HOLY SHIT IT COMPILED LOSS FUNCTION WHAT\"\n",
    "\n",
    "# create parameter update expressions\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01,\n",
    "                                            momentum=0.9)\n",
    "print \"HOLY SHIT IT COMPILED UPDATES\"\n",
    "\n",
    "# compile training function that updates parameters and returns training loss\n",
    "train_fn = theano.function([sentence1, sentence2, mask1, mask2, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "# train_fn = theano.function([sentence1, sentence2, target_var], loss, updates=updates)\n",
    "print \"HOLY SHIT IT COMPILED TRAINING FUNCTION\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOLY SHIT IT EPOCHS!\n",
      "reading \n",
      "Epoch 1: Loss 1.09885\n",
      ">>> modelStore/0606-044142-E0.pkl\n",
      "Done.\n",
      "HOLY SHIT IT EPOCHS!\n",
      "reading \n",
      "Epoch 2: Loss 1.09884\n",
      ">>> modelStore/0606-051717-E1.pkl\n",
      "Done.\n",
      "HOLY SHIT IT EPOCHS!\n",
      "reading \n",
      "Epoch 3: Loss 1.09884\n",
      ">>> modelStore/0606-055248-E2.pkl\n",
      "Done.\n",
      "HOLY SHIT IT EPOCHS!\n",
      "reading \n",
      "Epoch 4: Loss 1.09884\n",
      ">>> modelStore/0606-062807-E3.pkl\n",
      "Done.\n",
      "HOLY SHIT IT EPOCHS!\n",
      "reading \n",
      "Epoch 5: Loss 1.09884\n",
      ">>> modelStore/0606-070322-E4.pkl\n",
      "Done.\n",
      "HOLY SHIT IT EPOCHS!\n",
      "reading \n",
      "Epoch 6: Loss 1.09884\n",
      ">>> modelStore/0606-073837-E5.pkl\n",
      "Done.\n",
      "HOLY SHIT IT EPOCHS!\n",
      "reading \n",
      "Epoch 7: Loss 1.09884\n",
      ">>> modelStore/0606-081351-E6.pkl\n",
      "Done.\n",
      "HOLY SHIT IT EPOCHS!\n",
      "reading \n",
      "Epoch 8: Loss 1.09883\n",
      ">>> modelStore/0606-084906-E7.pkl\n",
      "Done.\n",
      "HOLY SHIT IT EPOCHS!\n",
      "reading \n",
      "Epoch 9: Loss 1.09883\n",
      ">>> modelStore/0606-092421-E8.pkl\n",
      "Done.\n",
      "HOLY SHIT IT EPOCHS!\n",
      "reading \n",
      "Epoch 10: Loss 1.09883\n",
      ">>> modelStore/0606-095935-E9.pkl\n",
      "Done.\n",
      "HOLY SHIT IT EPOCHS!\n",
      "reading \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cuda error 'an illegal instruction was encountered' while copying %lli data element to device memory. str ptr=%p. dst ptr=%p\nApply node that caused the error: GpuFromHost(Elemwise{Cast{float32}}.0)\nToposort index: 6\nInputs types: [TensorType(float32, col)]\nInputs shapes: [(6, 1)]\nInputs strides: [(4, 4)]\nInputs values: ['not shown']\nOutputs clients: [[GpuElemwise{Composite{Switch(i0, (scalar_sigmoid((i1 + (i2 * i3))) * tanh(i2)), i4)},no_inplace}(GpuFromHost.0, GpuSubtensor{::, int64:int64:}.0, GpuElemwise{Composite{((scalar_sigmoid((i0 + (i1 * i2))) * i1) + (scalar_sigmoid((i3 + (i1 * i4))) * tanh(i5)))},no_inplace}.0, <CudaNdarrayType(float32, row)>, <CudaNdarrayType(float32, matrix)>), GpuElemwise{Switch,no_inplace}(GpuFromHost.0, GpuElemwise{Composite{((scalar_sigmoid((i0 + (i1 * i2))) * i1) + (scalar_sigmoid((i3 + (i1 * i4))) * tanh(i5)))},no_inplace}.0, <CudaNdarrayType(float32, matrix)>)]]\n\nDebugprint of the apply node: \nGpuFromHost [id A] <CudaNdarrayType(float32, col)> ''   \n |Elemwise{Cast{float32}} [id B] <TensorType(float32, col)> ''   \n   |<TensorType(int8, col)> [id C] <TensorType(int8, col)>\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,gpu,scan_fn}(Elemwise{maximum,no_inplace}.0, GpuSubtensor{int64:int64:int8}.0, Subtensor{int64:int64:int8}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuJoin.0, GpuDimShuffle{x,0}.0, GpuDimShuffle{x,0}.0, GpuDimShuffle{x,0}.0)\nToposort index: 916\nInputs types: [TensorType(int64, scalar), CudaNdarrayType(float32, 3D), TensorType(int8, (False, False, True)), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, row), CudaNdarrayType(float32, row), CudaNdarrayType(float32, row)]\nInputs shapes: [(), (212, 6, 2048), (212, 6, 1), (213, 6, 512), (213, 6, 512), (512, 2048), (1, 512), (1, 512), (1, 512)]\nInputs strides: [(), (12288, 2048, 1), (1, 212, 1), (3072, 512, 1), (3072, 512, 1), (2048, 1), (0, 1), (0, 1), (0, 1)]\nInputs values: [array(212), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown']\nOutputs clients: [[GpuSubtensor{int64:int64:int64}(forall_inplace,gpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1})], [GpuSubtensor{int64:int64:int64}(forall_inplace,gpu,scan_fn}.1, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1}), GpuSubtensor{int64}(forall_inplace,gpu,scan_fn}.1, ScalarFromTensor.0)]]\n\nDebugprint of the apply node: \nforall_inplace,gpu,scan_fn}.0 [id A] <CudaNdarrayType(float32, 3D)> ''   \n |Elemwise{maximum,no_inplace} [id B] <TensorType(int64, scalar)> ''   \n | |Elemwise{Composite{minimum(maximum(maximum((i0 - i1), (i0 - i1)), ((i2 + i1) - i1)), i3)}} [id C] <TensorType(int64, scalar)> ''   \n | | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id D] <TensorType(int64, scalar)> ''   \n | | | |Elemwise{add,no_inplace} [id E] <TensorType(int64, scalar)> ''   \n | | | | |Elemwise{Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i2), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}} [id F] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{lt,no_inplace} [id G] <TensorType(int8, scalar)> ''   \n | | | | | | |Elemwise{Composite{Switch(i0, i1, maximum(minimum((i2 + i3), i4), i5))}} [id H] <TensorType(int64, scalar)> ''   \n | | | | | | | |Elemwise{le,no_inplace} [id I] <TensorType(int8, scalar)> ''   \n | | | | | | | | |Elemwise{sub,no_inplace} [id J] <TensorType(int64, scalar)> ''   \n | | | | | | | | | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id K] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |Elemwise{add,no_inplace} [id L] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | | | | | | | |Elemwise{Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, maximum(i2, i3))}(i0, i1, i2, i3), i1, i4), i1, i5), i4), i6, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, maximum(i2, i3))}(i0, i1, i2, i3), i1, i4), i1, i5))}} [id N] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   |Elemwise{le,no_inplace} [id O] <TensorType(int8, scalar)> ''   \n | | | | | | | | | | |   | |Elemwise{Composite{Switch(i0, Switch(LT(Composite{((i0 + i1) - i2)}(i1, i2, i3), i4), i4, Composite{((i0 + i1) - i2)}(i1, i2, i3)), Switch(LT(i1, (i2 - i3)), i1, (i2 - i3)))}} [id P] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | |Elemwise{lt,no_inplace} [id Q] <TensorType(int8, scalar)> ''   \n | | | | | | | | | | |   | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | | |Shape_i{1} [id S] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | | | |s1 [id T] <TensorType(float32, 3D)>\n | | | | | | | | | | |   | | | | |Shape_i{1} [id U] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | |   |m1 [id V] <TensorType(int8, matrix)>\n | | | | | | | | | | |   | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id X] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | | |Elemwise{lt,no_inplace} [id Q] <TensorType(int8, scalar)> ''   \n | | | | | | | | | | |   | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   | | |Elemwise{Composite{Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}((i0 - i1), i2, i3), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}((i0 - i1), i2, i3), i2), i1)}} [id Z] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id X] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   | | | |Elemwise{add,no_inplace} [id BA] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | |   |TensorConstant{-1} [id BB] <TensorType(int64, scalar)>\n | | | | | | | | | | |   | | |   |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   |Elemwise{Composite{Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}((i0 - i1), i2, i3), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}((i0 - i1), i2, i3), i2), i1)}} [id Z] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   |Elemwise{add,no_inplace} [id BC] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | |TensorConstant{-1} [id BB] <TensorType(int64, scalar)>\n | | | | | | | | | | |   | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id X] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   |TensorConstant{-1} [id BD] <TensorType(int8, scalar)>\n | | | | | | | | | | |   |Elemwise{add,no_inplace} [id BA] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | |Elemwise{Composite{Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{(i0 - Switch(LT(i1, i2), i2, i1))}(i0, Composite{(i0 - Switch(GE(i1, i2), i2, i1))}(i1, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, Switch(AND(LT(i2, i1), GT(i3, i1)), (i4 - i5), maximum((i4 + i6), i2)))}(i2, i3, (i4 - i5), i5, i6, i7, i8), i3, i7), i3, i9), i7), i3), i3, i1), i3), i10), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{(i0 - Switch(LT(i1, i2), i2, i1))}(i0, Composite{(i0 - Switch(GE(i1, i2), i2, i1))}(i1, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, Switch(AND(LT(i2, i1), GT(i3, i1)), (i4 - i5), maximum((i4 + i6), i2)))}(i2, i3, (i4 - i5), i5, i6, i7, i8), i3, i7), i3, i9), i7), i3), i3, i1), i3), i10)}} [id BE] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |Elemwise{add,no_inplace} [id L] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |Elemwise{Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, maximum(i2, i3))}(i0, i1, i2, i3), i1, i4), i1, i5), i4), i6, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, maximum(i2, i3))}(i0, i1, i2, i3), i1, i4), i1, i5))}} [id N] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |Elemwise{le,no_inplace} [id O] <TensorType(int8, scalar)> ''   \n | | | | | | | | |   |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | |   |Elemwise{add,no_inplace} [id BC] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |Elemwise{Composite{Switch(i0, Switch(LT(Composite{((i0 + i1) - i2)}(i1, i2, i3), i4), i4, Composite{((i0 + i1) - i2)}(i1, i2, i3)), Switch(LT(i1, (i2 - i3)), i1, (i2 - i3)))}} [id P] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |TensorConstant{-1} [id BB] <TensorType(int64, scalar)>\n | | | | | | | | |   |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |Elemwise{Composite{Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}((i0 - i1), i2, i3), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}((i0 - i1), i2, i3), i2), i1)}} [id Z] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |TensorConstant{-1} [id BD] <TensorType(int8, scalar)>\n | | | | | | | | |   |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id K] <TensorType(int64, scalar)> ''   \n | | | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | |TensorConstant{-1} [id BB] <TensorType(int64, scalar)>\n | | | | | | | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id K] <TensorType(int64, scalar)> ''   \n | | | | | | | |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | |TensorConstant{0} [id BF] <TensorType(int64, scalar)>\n | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | |Elemwise{Composite{Switch(i0, i1, maximum(minimum((i2 + i3), i4), i5))}} [id H] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | |TensorConstant{-1} [id BD] <TensorType(int8, scalar)>\n | | | | | |Elemwise{add,no_inplace} [id BG] <TensorType(int64, scalar)> ''   \n | | | | |   |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | |   |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | |Elemwise{Composite{Switch(i0, (i1 + i2 + i3), i1)}} [id BH] <TensorType(int64, scalar)> ''   \n | | | |Elemwise{lt,no_inplace} [id BI] <TensorType(int8, scalar)> ''   \n | | | | |Elemwise{Composite{Switch(LT((i0 - i1), i2), (i3 + (-i4)), Switch(GE((i0 - i1), i5), (i6 + i0), Switch(LE(i5, i2), (i6 + i0), i0)))}} [id BJ] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{Composite{Switch(LT(i0, i1), i0, i1)}} [id BK] <TensorType(int64, scalar)> ''   \n | | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | | |Elemwise{add,no_inplace} [id BG] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | |TensorConstant{-1} [id BB] <TensorType(int64, scalar)>\n | | | | | |Elemwise{add,no_inplace} [id BG] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{sub,no_inplace} [id BL] <TensorType(int64, scalar)> ''   \n | | | | | | |Elemwise{add,no_inplace} [id BG] <TensorType(int64, scalar)> ''   \n | | | | | | |Elemwise{Composite{Switch(LT(i0, i1), i0, i1)}} [id BK] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{2} [id BM] <TensorType(int64, scalar)>\n | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | |Elemwise{Composite{Switch(LT((i0 - i1), i2), (i3 + (-i4)), Switch(GE((i0 - i1), i5), (i6 + i0), Switch(LE(i5, i2), (i6 + i0), i0)))}} [id BJ] <TensorType(int64, scalar)> ''   \n | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n |GpuSubtensor{int64:int64:int8} [id BN] <CudaNdarrayType(float32, 3D)> ''   \n | |GpuElemwise{add,no_inplace} [id BO] <CudaNdarrayType(float32, 3D)> ''   \n | | |GpuReshape{3} [id BP] <CudaNdarrayType(float32, 3D)> ''   \n | | | |GpuDot22 [id BQ] <CudaNdarrayType(float32, matrix)> ''   \n | | | | |GpuReshape{2} [id BR] <CudaNdarrayType(float32, matrix)> ''   \n | | | | | |GpuDimShuffle{1,0,2} [id BS] <CudaNdarrayType(float32, 3D)> ''   \n | | | | | | |GpuFromHost [id BT] <CudaNdarrayType(float32, 3D)> ''   \n | | | | | |   |s1 [id T] <TensorType(float32, 3D)>\n | | | | | |MakeVector{dtype='int64'} [id BU] <TensorType(int64, vector)> ''   \n | | | | |   |Elemwise{mul,no_inplace} [id BV] <TensorType(int64, scalar)> ''   \n | | | | |   | |Shape_i{1} [id S] <TensorType(int64, scalar)> ''   \n | | | | |   | |Shape_i{0} [id BW] <TensorType(int64, scalar)> ''   \n | | | | |   |   |s1 [id T] <TensorType(float32, 3D)>\n | | | | |   |Shape_i{2} [id BX] <TensorType(int64, scalar)> ''   \n | | | | |     |s1 [id T] <TensorType(float32, 3D)>\n | | | | |GpuReshape{2} [id BY] <CudaNdarrayType(float32, matrix)> ''   \n | | | |   |GpuJoin [id BZ] <CudaNdarrayType(float32, matrix)> ''   \n | | | |   | |TensorConstant{1} [id CA] <TensorType(int8, scalar)>\n | | | |   | |W_in_to_ingate [id CB] <CudaNdarrayType(float32, matrix)>\n | | | |   | |W_in_to_forgetgate [id CC] <CudaNdarrayType(float32, matrix)>\n | | | |   | |W_in_to_cell [id CD] <CudaNdarrayType(float32, matrix)>\n | | | |   | |W_in_to_outgate [id CE] <CudaNdarrayType(float32, matrix)>\n | | | |   |MakeVector{dtype='int64'} [id CF] <TensorType(int64, vector)> ''   \n | | | |     |Shape_i{0} [id CG] <TensorType(int64, scalar)> ''   \n | | | |     | |W_in_to_ingate [id CB] <CudaNdarrayType(float32, matrix)>\n | | | |     |Elemwise{add,no_inplace} [id CH] <TensorType(int64, scalar)> ''   \n | | | |       |Shape_i{1} [id CI] <TensorType(int64, scalar)> ''   \n | | | |       | |W_in_to_outgate [id CE] <CudaNdarrayType(float32, matrix)>\n | | | |       |Shape_i{1} [id CJ] <TensorType(int64, scalar)> ''   \n | | | |       | |W_in_to_cell [id CD] <CudaNdarrayType(float32, matrix)>\n | | | |       |Shape_i{1} [id CK] <TensorType(int64, scalar)> ''   \n | | | |       | |W_in_to_ingate [id CB] <CudaNdarrayType(float32, matrix)>\n | | | |       |Shape_i{1} [id CL] <TensorType(int64, scalar)> ''   \n | | | |         |W_in_to_forgetgate [id CC] <CudaNdarrayType(float32, matrix)>\n | | | |MakeVector{dtype='int64'} [id CM] <TensorType(int64, vector)> ''   \n | | |   |Shape_i{1} [id S] <TensorType(int64, scalar)> ''   \n | | |   |Shape_i{0} [id BW] <TensorType(int64, scalar)> ''   \n | | |   |Elemwise{add,no_inplace} [id CH] <TensorType(int64, scalar)> ''   \n | | |GpuDimShuffle{x,x,0} [id CN] <CudaNdarrayType(float32, (True, True, False))> ''   \n | |   |GpuJoin [id CO] <CudaNdarrayType(float32, vector)> ''   \n | |     |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |     |b_ingate [id CP] <CudaNdarrayType(float32, vector)>\n | |     |b_forgetgate [id CQ] <CudaNdarrayType(float32, vector)>\n | |     |b_cell [id CR] <CudaNdarrayType(float32, vector)>\n | |     |b_outgate [id CS] <CudaNdarrayType(float32, vector)>\n | |ScalarFromTensor [id CT] <int64> ''   \n | | |Elemwise{switch,no_inplace} [id CU] <TensorType(int64, scalar)> ''   \n | |   |Elemwise{le,no_inplace} [id CV] <TensorType(int8, scalar)> ''   \n | |   | |Elemwise{Composite{Switch(i0, Switch(LT((i1 + i2), i3), i3, (i1 + i2)), Switch(LT(i1, i2), i1, i2))}} [id CW] <TensorType(int64, scalar)> ''   \n | |   | | |Elemwise{lt,no_inplace} [id Q] <TensorType(int8, scalar)> ''   \n | |   | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | |   | | |Shape_i{1} [id S] <TensorType(int64, scalar)> ''   \n | |   | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   |TensorConstant{0} [id BF] <TensorType(int64, scalar)>\n | |ScalarFromTensor [id CX] <int64> ''   \n | | |Elemwise{Composite{Switch(i0, i1, minimum(i2, i3))}} [id CY] <TensorType(int64, scalar)> ''   \n | |   |Elemwise{le,no_inplace} [id CV] <TensorType(int8, scalar)> ''   \n | |   |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   |Elemwise{Composite{Switch(i0, Switch(LT((i1 + i2), i3), i3, (i1 + i2)), Switch(LT(i1, i2), i1, i2))}} [id CW] <TensorType(int64, scalar)> ''   \n | |   |Shape_i{1} [id S] <TensorType(int64, scalar)> ''   \n | |Constant{1} [id CZ] <int8>\n |Subtensor{int64:int64:int8} [id DA] <TensorType(int8, (False, False, True))> ''   \n | |InplaceDimShuffle{1,0,x} [id DB] <TensorType(int8, (False, False, True))> ''   \n | | |m1 [id V] <TensorType(int8, matrix)>\n | |ScalarFromTensor [id DC] <int64> ''   \n | | |Elemwise{switch,no_inplace} [id DD] <TensorType(int64, scalar)> ''   \n | |   |Elemwise{le,no_inplace} [id DE] <TensorType(int8, scalar)> ''   \n | |   | |Elemwise{Composite{Switch(i0, Switch(LT((i1 + i2), i3), i3, (i1 + i2)), Switch(LT(i1, i2), i1, i2))}} [id DF] <TensorType(int64, scalar)> ''   \n | |   | | |Elemwise{lt,no_inplace} [id Q] <TensorType(int8, scalar)> ''   \n | |   | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | |   | | |Shape_i{1} [id U] <TensorType(int64, scalar)> ''   \n | |   | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   |TensorConstant{0} [id BF] <TensorType(int64, scalar)>\n | |ScalarFromTensor [id DG] <int64> ''   \n | | |Elemwise{Composite{Switch(i0, i1, minimum(i2, i3))}} [id DH] <TensorType(int64, scalar)> ''   \n | |   |Elemwise{le,no_inplace} [id DE] <TensorType(int8, scalar)> ''   \n | |   |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   |Elemwise{Composite{Switch(i0, Switch(LT((i1 + i2), i3), i3, (i1 + i2)), Switch(LT(i1, i2), i1, i2))}} [id DF] <TensorType(int64, scalar)> ''   \n | |   |Shape_i{1} [id U] <TensorType(int64, scalar)> ''   \n | |Constant{1} [id CZ] <int8>\n |GpuIncSubtensor{InplaceSet;:int64:} [id DI] <CudaNdarrayType(float32, 3D)> ''   \n | |GpuAllocEmpty [id DJ] <CudaNdarrayType(float32, 3D)> ''   \n | | |Elemwise{add,no_inplace} [id DK] <TensorType(int64, scalar)> ''   \n | | | |Elemwise{Composite{Switch(LT(maximum(i0, i1), i2), (maximum(i0, i1) + i3), (maximum(i0, i1) - i3))}} [id DL] <TensorType(int64, scalar)> ''   \n | | | | |Elemwise{Composite{((i0 - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}(i1, Composite{(((i0 - i1) // i1) + i1)}(i2, i3), i4, i3), i4, i1, i3), i4), i5), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}(i1, Composite{(((i0 - i1) // i1) + i1)}(i2, i3), i4, i3), i4, i1, i3), i4), i5)) + i3)}} [id DM] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{maximum,no_inplace} [id B] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{add,no_inplace} [id E] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{Composite{(i0 - Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i1, i2, i3, i4), i5, i6), i7), i7, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i1, i2, i3, i4), i5, i6)))}} [id DN] <TensorType(int64, scalar)> ''   \n | | | | | | |Elemwise{Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i2), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}} [id F] <TensorType(int64, scalar)> ''   \n | | | | | | |Elemwise{lt,no_inplace} [id DO] <TensorType(int8, scalar)> ''   \n | | | | | | | |Elemwise{Composite{Switch(i0, i1, Switch(AND(LT((i2 + i3), i1), GT(i4, i1)), (i5 - i6), minimum((i2 + i3), i7)))}} [id DP] <TensorType(int64, scalar)> ''   \n | | | | | | | | |Elemwise{le,no_inplace} [id I] <TensorType(int8, scalar)> ''   \n | | | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | |TensorConstant{-1} [id BB] <TensorType(int64, scalar)>\n | | | | | | | | |Elemwise{Composite{Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{(i0 - Switch(LT(i1, i2), i2, i1))}(i0, Composite{(i0 - Switch(GE(i1, i2), i2, i1))}(i1, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, Switch(AND(LT(i2, i1), GT(i3, i1)), (i4 - i5), maximum((i4 + i6), i2)))}(i2, i3, (i4 - i5), i5, i6, i7, i8), i3, i7), i3, i9), i7), i3), i3, i1), i3), i10), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{(i0 - Switch(LT(i1, i2), i2, i1))}(i0, Composite{(i0 - Switch(GE(i1, i2), i2, i1))}(i1, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, Switch(AND(LT(i2, i1), GT(i3, i1)), (i4 - i5), maximum((i4 + i6), i2)))}(i2, i3, (i4 - i5), i5, i6, i7, i8), i3, i7), i3, i9), i7), i3), i3, i1), i3), i10)}} [id BE] <TensorType(int64, scalar)> ''   \n | | | | | | | | |Elemwise{sub,no_inplace} [id J] <TensorType(int64, scalar)> ''   \n | | | | | | | | |TensorConstant{-2} [id DQ] <TensorType(int64, scalar)>\n | | | | | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | | | | |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | |Elemwise{Composite{Switch(i0, i1, Switch(AND(LT((i2 + i3), i1), GT(i4, i1)), (i5 - i6), minimum((i2 + i3), i7)))}} [id DP] <TensorType(int64, scalar)> ''   \n | | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | |TensorConstant{-1} [id BD] <TensorType(int8, scalar)>\n | | | | | | |Elemwise{add,no_inplace} [id BG] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id D] <TensorType(int64, scalar)> ''   \n | | | | |TensorConstant{2} [id BM] <TensorType(int64, scalar)>\n | | | | |TensorConstant{1} [id CA] <TensorType(int8, scalar)>\n | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | |Shape_i{0} [id BW] <TensorType(int64, scalar)> ''   \n | | |Shape_i{1} [id DR] <TensorType(int64, scalar)> ''   \n | |   |cell_init [id DS] <CudaNdarrayType(float32, matrix)>\n | |Rebroadcast{0} [id DT] <CudaNdarrayType(float32, 3D)> ''   \n | | |GpuDimShuffle{x,0,1} [id DU] <CudaNdarrayType(float32, (True, False, False))> ''   \n | |   |GpuDot22 [id DV] <CudaNdarrayType(float32, matrix)> ''   \n | |     |GpuAlloc [id DW] <CudaNdarrayType(float32, col)> ''   \n | |     | |CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host} [id DX] <CudaNdarrayType(float32, scalar)>\n | |     | |Shape_i{0} [id BW] <TensorType(int64, scalar)> ''   \n | |     | |TensorConstant{1} [id CA] <TensorType(int8, scalar)>\n | |     |cell_init [id DS] <CudaNdarrayType(float32, matrix)>\n | |Constant{1} [id DY] <int64>\n |GpuIncSubtensor{InplaceSet;:int64:} [id DZ] <CudaNdarrayType(float32, 3D)> ''   \n | |GpuAllocEmpty [id EA] <CudaNdarrayType(float32, 3D)> ''   \n | | |Elemwise{add,no_inplace} [id EB] <TensorType(int64, scalar)> ''   \n | | | |Elemwise{Composite{Switch(LT(i0, i1), (i0 + i2), (i0 - i2))}} [id EC] <TensorType(int64, scalar)> ''   \n | | | | |Elemwise{Composite{maximum(maximum(((i0 - Switch(i1, (i2 + i3 + i4), i2)) + i4), i5), maximum(((i0 - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), Composite{((((i0 - Switch(GE(i1, i2), i2, i1)) - i3) // i3) + i3)}(Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i11, i12, i3, i4), i8, i9), i10, i4), i8, i4), i8, (Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i4), i8), Composite{Switch(LT(i0, i1), i1, i0)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i8)), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), Composite{((((i0 - Switch(GE(i1, i2), i2, i1)) - i3) // i3) + i3)}(Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i11, i12, i3, i4), i8, i9), i10, i4), i8, i4), i8, (Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i4), i8), Composite{Switch(LT(i0, i1), i1, i0)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i8))) + i4), i5))}} [id ED] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{maximum,no_inplace} [id B] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{lt,no_inplace} [id BI] <TensorType(int8, scalar)> ''   \n | | | | | |Elemwise{Composite{Switch(LT((i0 - i1), i2), (i3 + (-i4)), Switch(GE((i0 - i1), i5), (i6 + i0), Switch(LE(i5, i2), (i6 + i0), i0)))}} [id BJ] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | |TensorConstant{2} [id BM] <TensorType(int64, scalar)>\n | | | | | |Elemwise{lt,no_inplace} [id G] <TensorType(int8, scalar)> ''   \n | | | | | |Elemwise{Composite{Switch(i0, i1, maximum(minimum((i2 + i3), i4), i5))}} [id H] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | |TensorConstant{-1} [id BD] <TensorType(int8, scalar)>\n | | | | | |Elemwise{add,no_inplace} [id EE] <TensorType(int64, scalar)> ''   \n | | | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | |Elemwise{lt,no_inplace} [id DO] <TensorType(int8, scalar)> ''   \n | | | | | |Elemwise{Composite{Switch(i0, i1, Switch(AND(LT((i2 + i3), i1), GT(i4, i1)), (i5 - i6), minimum((i2 + i3), i7)))}} [id DP] <TensorType(int64, scalar)> ''   \n | | | | |TensorConstant{1} [id CA] <TensorType(int8, scalar)>\n | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | |Shape_i{0} [id BW] <TensorType(int64, scalar)> ''   \n | | |Shape_i{1} [id EF] <TensorType(int64, scalar)> ''   \n | |   |hid_init [id EG] <CudaNdarrayType(float32, matrix)>\n | |Rebroadcast{0} [id EH] <CudaNdarrayType(float32, 3D)> ''   \n | | |GpuDimShuffle{x,0,1} [id EI] <CudaNdarrayType(float32, (True, False, False))> ''   \n | |   |GpuDot22 [id EJ] <CudaNdarrayType(float32, matrix)> ''   \n | |     |GpuAlloc [id DW] <CudaNdarrayType(float32, col)> ''   \n | |     |hid_init [id EG] <CudaNdarrayType(float32, matrix)>\n | |Constant{1} [id DY] <int64>\n |GpuJoin [id EK] <CudaNdarrayType(float32, matrix)> ''   \n | |TensorConstant{1} [id CA] <TensorType(int8, scalar)>\n | |W_hid_to_ingate [id EL] <CudaNdarrayType(float32, matrix)>\n | |W_hid_to_forgetgate [id EM] <CudaNdarrayType(float32, matrix)>\n | |W_hid_to_cell [id EN] <CudaNdarrayType(float32, matrix)>\n | |W_hid_to_outgate [id EO] <CudaNdarrayType(float32, matrix)>\n |GpuDimShuffle{x,0} [id EP] <CudaNdarrayType(float32, row)> ''   \n | |W_cell_to_ingate [id EQ] <CudaNdarrayType(float32, vector)>\n |GpuDimShuffle{x,0} [id ER] <CudaNdarrayType(float32, row)> ''   \n | |W_cell_to_forgetgate [id ES] <CudaNdarrayType(float32, vector)>\n |GpuDimShuffle{x,0} [id ET] <CudaNdarrayType(float32, row)> ''   \n   |W_cell_to_outgate [id EU] <CudaNdarrayType(float32, vector)>\nforall_inplace,gpu,scan_fn}.1 [id A] <CudaNdarrayType(float32, 3D)> ''   \n\nInner graphs of the scan ops:\n\nforall_inplace,gpu,scan_fn}.0 [id A] <CudaNdarrayType(float32, 3D)> ''   \n >GpuElemwise{Switch,no_inplace} [id EV] <CudaNdarrayType(float32, matrix)> ''   \n > |GpuFromHost [id EW] <CudaNdarrayType(float32, col)> ''   \n > | |Elemwise{Cast{float32}} [id EX] <TensorType(float32, col)> ''   \n > |   |<TensorType(int8, col)> [id EY] <TensorType(int8, col)> -> [id DA]\n > |GpuElemwise{Composite{((scalar_sigmoid((i0 + (i1 * i2))) * i1) + (scalar_sigmoid((i3 + (i1 * i4))) * tanh(i5)))},no_inplace} [id EZ] <CudaNdarrayType(float32, matrix)> ''   \n > | |GpuSubtensor{::, int64:int64:} [id FA] <CudaNdarrayType(float32, matrix)> ''   \n > | | |GpuGemm{no_inplace} [id FB] <CudaNdarrayType(float32, matrix)> ''   \n > | | | |<CudaNdarrayType(float32, matrix)> [id FC] <CudaNdarrayType(float32, matrix)> -> [id BN]\n > | | | |TensorConstant{1.0} [id FD] <TensorType(float32, scalar)>\n > | | | |<CudaNdarrayType(float32, matrix)> [id FE] <CudaNdarrayType(float32, matrix)> -> [id DZ]\n > | | | |<CudaNdarrayType(float32, matrix)> [id FF] <CudaNdarrayType(float32, matrix)> -> [id EK]\n > | | | |TensorConstant{1.0} [id FD] <TensorType(float32, scalar)>\n > | | |Constant{512} [id FG] <int64>\n > | | |Constant{1024} [id FH] <int64>\n > | |<CudaNdarrayType(float32, matrix)> [id FI] <CudaNdarrayType(float32, matrix)> -> [id DI]\n > | |<CudaNdarrayType(float32, row)> [id FJ] <CudaNdarrayType(float32, row)> -> [id ER]\n > | |GpuSubtensor{::, int64:int64:} [id FK] <CudaNdarrayType(float32, matrix)> ''   \n > | | |GpuGemm{no_inplace} [id FB] <CudaNdarrayType(float32, matrix)> ''   \n > | | |Constant{0} [id FL] <int64>\n > | | |Constant{512} [id FG] <int64>\n > | |<CudaNdarrayType(float32, row)> [id FM] <CudaNdarrayType(float32, row)> -> [id EP]\n > | |GpuSubtensor{::, int64:int64:} [id FN] <CudaNdarrayType(float32, matrix)> ''   \n > |   |GpuGemm{no_inplace} [id FB] <CudaNdarrayType(float32, matrix)> ''   \n > |   |Constant{1024} [id FH] <int64>\n > |   |Constant{1536} [id FO] <int64>\n > |<CudaNdarrayType(float32, matrix)> [id FI] <CudaNdarrayType(float32, matrix)> -> [id DI]\n >GpuElemwise{Composite{Switch(i0, (scalar_sigmoid((i1 + (i2 * i3))) * tanh(i2)), i4)},no_inplace} [id FP] <CudaNdarrayType(float32, matrix)> ''   \n > |GpuFromHost [id EW] <CudaNdarrayType(float32, col)> ''   \n > |GpuSubtensor{::, int64:int64:} [id FQ] <CudaNdarrayType(float32, matrix)> ''   \n > | |GpuGemm{no_inplace} [id FB] <CudaNdarrayType(float32, matrix)> ''   \n > | |Constant{1536} [id FO] <int64>\n > | |Constant{2048} [id FR] <int64>\n > |GpuElemwise{Composite{((scalar_sigmoid((i0 + (i1 * i2))) * i1) + (scalar_sigmoid((i3 + (i1 * i4))) * tanh(i5)))},no_inplace} [id EZ] <CudaNdarrayType(float32, matrix)> ''   \n > |<CudaNdarrayType(float32, row)> [id FS] <CudaNdarrayType(float32, row)> -> [id ET]\n > |<CudaNdarrayType(float32, matrix)> [id FE] <CudaNdarrayType(float32, matrix)> -> [id DZ]\n\nforall_inplace,gpu,scan_fn}.1 [id A] <CudaNdarrayType(float32, 3D)> ''   \n >GpuElemwise{Switch,no_inplace} [id EV] <CudaNdarrayType(float32, matrix)> ''   \n >GpuElemwise{Composite{Switch(i0, (scalar_sigmoid((i1 + (i2 * i3))) * tanh(i2)), i4)},no_inplace} [id FP] <CudaNdarrayType(float32, matrix)> ''   \n\nStorage map footprint:\n - GpuElemwise{add,no_inplace}.0, Shape: (212, 6, 2048), ElemSize: 4 Byte(s), TotalSize: 10420224 Byte(s)\n - GpuSubtensor{int64:int64:int8}.0, Shape: (212, 6, 2048), ElemSize: 4 Byte(s), TotalSize: 10420224 Byte(s)\n - GpuJoin.0, Shape: (512, 2048), ElemSize: 4 Byte(s), TotalSize: 4194304 Byte(s)\n - forall_inplace,gpu,scan_fn}.1, Shape: (213, 6, 512), ElemSize: 4 Byte(s), TotalSize: 2617344 Byte(s)\n - GpuIncSubtensor{InplaceSet;:int64:}.0, Shape: (213, 6, 512), ElemSize: 4 Byte(s), TotalSize: 2617344 Byte(s)\n - GpuIncSubtensor{InplaceSet;:int64:}.0, Shape: (213, 6, 512), ElemSize: 4 Byte(s), TotalSize: 2617344 Byte(s)\n - forall_inplace,gpu,scan_fn}.0, Shape: (213, 6, 512), ElemSize: 4 Byte(s), TotalSize: 2617344 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (2550, 128), ElemSize: 4 Byte(s), TotalSize: 1305600 Byte(s)\n - W, Shared Input, Shape: (2550, 128), ElemSize: 4 Byte(s), TotalSize: 1305600 Byte(s)\n - W_hid_to_ingate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_forgetgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_cell, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_ingate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_outgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_forgetgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_outgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_forgetgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_forgetgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_cell, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_outgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_ingate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_outgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_cell, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_ingate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_cell, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - GpuReshape{2}.0, Shape: (1272, 30), ElemSize: 4 Byte(s), TotalSize: 152640 Byte(s)\n - s1, Input, Shape: (6, 212, 30), ElemSize: 4 Byte(s), TotalSize: 152640 Byte(s)\n - s2, Input, Shape: (6, 147, 30), ElemSize: 4 Byte(s), TotalSize: 105840 Byte(s)\n - W_in_to_cell, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_forgetgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_cell, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_ingate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_ingate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_forgetgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_outgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_forgetgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_ingate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_cell, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_outgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_ingate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_forgetgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_outgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_cell, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_outgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, 4D)>, Shared Input, Shape: (10, 20, 3, 3), ElemSize: 4 Byte(s), TotalSize: 7200 Byte(s)\n - W, Shared Input, Shape: (10, 20, 3, 3), ElemSize: 4 Byte(s), TotalSize: 7200 Byte(s)\n - cell_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - GpuDimShuffle{x,0}.0, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_cell, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - hid_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - GpuDimShuffle{x,0}.0, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - GpuDimShuffle{x,0}.0, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_cell, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - cell_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - hid_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_cell, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - cell_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - hid_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_cell, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - cell_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - hid_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (128, 3), ElemSize: 4 Byte(s), TotalSize: 1536 Byte(s)\n - W, Shared Input, Shape: (128, 3), ElemSize: 4 Byte(s), TotalSize: 1536 Byte(s)\n - InplaceDimShuffle{1,0,x}.0, Shape: (212, 6, 1), ElemSize: 1 Byte(s), TotalSize: 1272 Byte(s)\n - m1, Input, Shape: (6, 212), ElemSize: 1 Byte(s), TotalSize: 1272 Byte(s)\n - Subtensor{int64:int64:int8}.0, Shape: (212, 6, 1), ElemSize: 1 Byte(s), TotalSize: 1272 Byte(s)\n - m2, Input, Shape: (6, 147), ElemSize: 1 Byte(s), TotalSize: 882 Byte(s)\n - <CudaNdarrayType(float32, 4D)>, Shared Input, Shape: (20, 1, 3, 3), ElemSize: 4 Byte(s), TotalSize: 720 Byte(s)\n - W, Shared Input, Shape: (20, 1, 3, 3), ElemSize: 4 Byte(s), TotalSize: 720 Byte(s)\n - b, Shared Input, Shape: (128,), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (128,), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (20,), ElemSize: 4 Byte(s), TotalSize: 80 Byte(s)\n - b, Shared Input, Shape: (20,), ElemSize: 4 Byte(s), TotalSize: 80 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (10,), ElemSize: 4 Byte(s), TotalSize: 40 Byte(s)\n - b, Shared Input, Shape: (10,), ElemSize: 4 Byte(s), TotalSize: 40 Byte(s)\n - TensorConstant{[ -1   1   4 512]}, Shape: (4,), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - GpuAlloc.0, Shape: (6, 1), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)\n - ent, Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)\n - TensorConstant{(2,) of 4}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{(2,) of 2}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{(2,) of 0}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - b, Shared Input, Shape: (3,), ElemSize: 4 Byte(s), TotalSize: 12 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (3,), ElemSize: 4 Byte(s), TotalSize: 12 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{4}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{512}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{20}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{minimum(maximum(maximum((i0 - i1), (i0 - i1)), ((i2 + i1) - i1)), i3)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(i0, Switch(LT((i1 + i2), i3), i3, (i1 + i2)), Switch(LT(i1, i2), i1, i2))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{maximum,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(i0, Switch(LT((i1 + i2), i3), i3, (i1 + i2)), Switch(LT(i1, i2), i1, i2))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{add,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{-2}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{((i0 - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}(i1, Composite{(((i0 - i1) // i1) + i1)}(i2, i3), i4, i3), i4, i1, i3), i4), i5), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}(i1, Composite{(((i0 - i1) // i1) + i1)}(i2, i3), i4, i3), i4, i1, i3), i4), i5)) + i3)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{2550}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{-1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{sub,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{2}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(LT(i0, i1), (i0 + i2), (i0 - i2))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{2048}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{add,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(i0, (i1 + i2 + i3), i1)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{10}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{add,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(LT(maximum(i0, i1), i2), (maximum(i0, i1) + i3), (maximum(i0, i1) - i3))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i2), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{2}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{minimum,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{add,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{(i0 - Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i1, i2, i3, i4), i5, i6), i7), i7, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i1, i2, i3, i4), i5, i6)))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{add,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{mul,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{-1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{maximum(maximum(((i0 - Switch(i1, (i2 + i3 + i4), i2)) + i4), i5), maximum(((i0 - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), Composite{((((i0 - Switch(GE(i1, i2), i2, i1)) - i3) // i3) + i3)}(Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i11, i12, i3, i4), i8, i9), i10, i4), i8, i4), i8, (Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i4), i8), Composite{Switch(LT(i0, i1), i1, i0)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i8)), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), Composite{((((i0 - Switch(GE(i1, i2), i2, i1)) - i3) // i3) + i3)}(Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i11, i12, i3, i4), i8, i9), i10, i4), i8, i4), i8, (Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i4), i8), Composite{Switch(LT(i0, i1), i1, i0)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i8))) + i4), i5))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{255}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{0.990000009537}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - TensorConstant{0.00999999977648}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{1.00999999046}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - Constant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Constant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - TensorConstant{1.99999999495e-06}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - Constant{0.01}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Constant{1}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{2}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{-1}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Elemwise{lt,no_inplace}.0, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n TotalSize: 59283384.0 Byte(s) 0.055 GB\n TotalSize inputs: 39281239.0 Byte(s) 0.037 GB\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-68e8cf7d0ba1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# print batch1.shape, batch2.shape, ys.shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# loss += train_fn(batch1, batch2, makeMask(batch1), makeMask(batch2), ys)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mbatchCount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatchCount\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    910\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    913\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/xiaonan/.theano/compiledir_Linux-4.2--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.11-64/scan_perform/mod.cpp:4316)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/xiaonan/.theano/compiledir_Linux-4.2--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.11-64/scan_perform/mod.cpp:4193)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cuda error 'an illegal instruction was encountered' while copying %lli data element to device memory. str ptr=%p. dst ptr=%p\nApply node that caused the error: GpuFromHost(Elemwise{Cast{float32}}.0)\nToposort index: 6\nInputs types: [TensorType(float32, col)]\nInputs shapes: [(6, 1)]\nInputs strides: [(4, 4)]\nInputs values: ['not shown']\nOutputs clients: [[GpuElemwise{Composite{Switch(i0, (scalar_sigmoid((i1 + (i2 * i3))) * tanh(i2)), i4)},no_inplace}(GpuFromHost.0, GpuSubtensor{::, int64:int64:}.0, GpuElemwise{Composite{((scalar_sigmoid((i0 + (i1 * i2))) * i1) + (scalar_sigmoid((i3 + (i1 * i4))) * tanh(i5)))},no_inplace}.0, <CudaNdarrayType(float32, row)>, <CudaNdarrayType(float32, matrix)>), GpuElemwise{Switch,no_inplace}(GpuFromHost.0, GpuElemwise{Composite{((scalar_sigmoid((i0 + (i1 * i2))) * i1) + (scalar_sigmoid((i3 + (i1 * i4))) * tanh(i5)))},no_inplace}.0, <CudaNdarrayType(float32, matrix)>)]]\n\nDebugprint of the apply node: \nGpuFromHost [id A] <CudaNdarrayType(float32, col)> ''   \n |Elemwise{Cast{float32}} [id B] <TensorType(float32, col)> ''   \n   |<TensorType(int8, col)> [id C] <TensorType(int8, col)>\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,gpu,scan_fn}(Elemwise{maximum,no_inplace}.0, GpuSubtensor{int64:int64:int8}.0, Subtensor{int64:int64:int8}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuJoin.0, GpuDimShuffle{x,0}.0, GpuDimShuffle{x,0}.0, GpuDimShuffle{x,0}.0)\nToposort index: 916\nInputs types: [TensorType(int64, scalar), CudaNdarrayType(float32, 3D), TensorType(int8, (False, False, True)), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, row), CudaNdarrayType(float32, row), CudaNdarrayType(float32, row)]\nInputs shapes: [(), (212, 6, 2048), (212, 6, 1), (213, 6, 512), (213, 6, 512), (512, 2048), (1, 512), (1, 512), (1, 512)]\nInputs strides: [(), (12288, 2048, 1), (1, 212, 1), (3072, 512, 1), (3072, 512, 1), (2048, 1), (0, 1), (0, 1), (0, 1)]\nInputs values: [array(212), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown']\nOutputs clients: [[GpuSubtensor{int64:int64:int64}(forall_inplace,gpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1})], [GpuSubtensor{int64:int64:int64}(forall_inplace,gpu,scan_fn}.1, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1}), GpuSubtensor{int64}(forall_inplace,gpu,scan_fn}.1, ScalarFromTensor.0)]]\n\nDebugprint of the apply node: \nforall_inplace,gpu,scan_fn}.0 [id A] <CudaNdarrayType(float32, 3D)> ''   \n |Elemwise{maximum,no_inplace} [id B] <TensorType(int64, scalar)> ''   \n | |Elemwise{Composite{minimum(maximum(maximum((i0 - i1), (i0 - i1)), ((i2 + i1) - i1)), i3)}} [id C] <TensorType(int64, scalar)> ''   \n | | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id D] <TensorType(int64, scalar)> ''   \n | | | |Elemwise{add,no_inplace} [id E] <TensorType(int64, scalar)> ''   \n | | | | |Elemwise{Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i2), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}} [id F] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{lt,no_inplace} [id G] <TensorType(int8, scalar)> ''   \n | | | | | | |Elemwise{Composite{Switch(i0, i1, maximum(minimum((i2 + i3), i4), i5))}} [id H] <TensorType(int64, scalar)> ''   \n | | | | | | | |Elemwise{le,no_inplace} [id I] <TensorType(int8, scalar)> ''   \n | | | | | | | | |Elemwise{sub,no_inplace} [id J] <TensorType(int64, scalar)> ''   \n | | | | | | | | | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id K] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |Elemwise{add,no_inplace} [id L] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | | | | | | | |Elemwise{Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, maximum(i2, i3))}(i0, i1, i2, i3), i1, i4), i1, i5), i4), i6, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, maximum(i2, i3))}(i0, i1, i2, i3), i1, i4), i1, i5))}} [id N] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   |Elemwise{le,no_inplace} [id O] <TensorType(int8, scalar)> ''   \n | | | | | | | | | | |   | |Elemwise{Composite{Switch(i0, Switch(LT(Composite{((i0 + i1) - i2)}(i1, i2, i3), i4), i4, Composite{((i0 + i1) - i2)}(i1, i2, i3)), Switch(LT(i1, (i2 - i3)), i1, (i2 - i3)))}} [id P] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | |Elemwise{lt,no_inplace} [id Q] <TensorType(int8, scalar)> ''   \n | | | | | | | | | | |   | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | | |Shape_i{1} [id S] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | | | |s1 [id T] <TensorType(float32, 3D)>\n | | | | | | | | | | |   | | | | |Shape_i{1} [id U] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | |   |m1 [id V] <TensorType(int8, matrix)>\n | | | | | | | | | | |   | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id X] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | | |Elemwise{lt,no_inplace} [id Q] <TensorType(int8, scalar)> ''   \n | | | | | | | | | | |   | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   | | |Elemwise{Composite{Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}((i0 - i1), i2, i3), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}((i0 - i1), i2, i3), i2), i1)}} [id Z] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id X] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   | | | |Elemwise{add,no_inplace} [id BA] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | |   |TensorConstant{-1} [id BB] <TensorType(int64, scalar)>\n | | | | | | | | | | |   | | |   |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | | |   |Elemwise{Composite{Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}((i0 - i1), i2, i3), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}((i0 - i1), i2, i3), i2), i1)}} [id Z] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   |Elemwise{add,no_inplace} [id BC] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   | |TensorConstant{-1} [id BB] <TensorType(int64, scalar)>\n | | | | | | | | | | |   | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id X] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |   |TensorConstant{-1} [id BD] <TensorType(int8, scalar)>\n | | | | | | | | | | |   |Elemwise{add,no_inplace} [id BA] <TensorType(int64, scalar)> ''   \n | | | | | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | | |Elemwise{Composite{Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{(i0 - Switch(LT(i1, i2), i2, i1))}(i0, Composite{(i0 - Switch(GE(i1, i2), i2, i1))}(i1, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, Switch(AND(LT(i2, i1), GT(i3, i1)), (i4 - i5), maximum((i4 + i6), i2)))}(i2, i3, (i4 - i5), i5, i6, i7, i8), i3, i7), i3, i9), i7), i3), i3, i1), i3), i10), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{(i0 - Switch(LT(i1, i2), i2, i1))}(i0, Composite{(i0 - Switch(GE(i1, i2), i2, i1))}(i1, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, Switch(AND(LT(i2, i1), GT(i3, i1)), (i4 - i5), maximum((i4 + i6), i2)))}(i2, i3, (i4 - i5), i5, i6, i7, i8), i3, i7), i3, i9), i7), i3), i3, i1), i3), i10)}} [id BE] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |Elemwise{add,no_inplace} [id L] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |Elemwise{Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, maximum(i2, i3))}(i0, i1, i2, i3), i1, i4), i1, i5), i4), i6, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, maximum(i2, i3))}(i0, i1, i2, i3), i1, i4), i1, i5))}} [id N] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |Elemwise{le,no_inplace} [id O] <TensorType(int8, scalar)> ''   \n | | | | | | | | |   |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | |   |Elemwise{add,no_inplace} [id BC] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |Elemwise{Composite{Switch(i0, Switch(LT(Composite{((i0 + i1) - i2)}(i1, i2, i3), i4), i4, Composite{((i0 + i1) - i2)}(i1, i2, i3)), Switch(LT(i1, (i2 - i3)), i1, (i2 - i3)))}} [id P] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |TensorConstant{-1} [id BB] <TensorType(int64, scalar)>\n | | | | | | | | |   |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |Elemwise{Composite{Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}((i0 - i1), i2, i3), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}((i0 - i1), i2, i3), i2), i1)}} [id Z] <TensorType(int64, scalar)> ''   \n | | | | | | | | |   |TensorConstant{-1} [id BD] <TensorType(int8, scalar)>\n | | | | | | | | |   |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id K] <TensorType(int64, scalar)> ''   \n | | | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | |TensorConstant{-1} [id BB] <TensorType(int64, scalar)>\n | | | | | | | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id K] <TensorType(int64, scalar)> ''   \n | | | | | | | |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | |TensorConstant{0} [id BF] <TensorType(int64, scalar)>\n | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | |Elemwise{Composite{Switch(i0, i1, maximum(minimum((i2 + i3), i4), i5))}} [id H] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | |TensorConstant{-1} [id BD] <TensorType(int8, scalar)>\n | | | | | |Elemwise{add,no_inplace} [id BG] <TensorType(int64, scalar)> ''   \n | | | | |   |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | |   |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | |Elemwise{Composite{Switch(i0, (i1 + i2 + i3), i1)}} [id BH] <TensorType(int64, scalar)> ''   \n | | | |Elemwise{lt,no_inplace} [id BI] <TensorType(int8, scalar)> ''   \n | | | | |Elemwise{Composite{Switch(LT((i0 - i1), i2), (i3 + (-i4)), Switch(GE((i0 - i1), i5), (i6 + i0), Switch(LE(i5, i2), (i6 + i0), i0)))}} [id BJ] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{Composite{Switch(LT(i0, i1), i0, i1)}} [id BK] <TensorType(int64, scalar)> ''   \n | | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | | |Elemwise{add,no_inplace} [id BG] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | |TensorConstant{-1} [id BB] <TensorType(int64, scalar)>\n | | | | | |Elemwise{add,no_inplace} [id BG] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{sub,no_inplace} [id BL] <TensorType(int64, scalar)> ''   \n | | | | | | |Elemwise{add,no_inplace} [id BG] <TensorType(int64, scalar)> ''   \n | | | | | | |Elemwise{Composite{Switch(LT(i0, i1), i0, i1)}} [id BK] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{2} [id BM] <TensorType(int64, scalar)>\n | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | |Elemwise{Composite{Switch(LT((i0 - i1), i2), (i3 + (-i4)), Switch(GE((i0 - i1), i5), (i6 + i0), Switch(LE(i5, i2), (i6 + i0), i0)))}} [id BJ] <TensorType(int64, scalar)> ''   \n | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n |GpuSubtensor{int64:int64:int8} [id BN] <CudaNdarrayType(float32, 3D)> ''   \n | |GpuElemwise{add,no_inplace} [id BO] <CudaNdarrayType(float32, 3D)> ''   \n | | |GpuReshape{3} [id BP] <CudaNdarrayType(float32, 3D)> ''   \n | | | |GpuDot22 [id BQ] <CudaNdarrayType(float32, matrix)> ''   \n | | | | |GpuReshape{2} [id BR] <CudaNdarrayType(float32, matrix)> ''   \n | | | | | |GpuDimShuffle{1,0,2} [id BS] <CudaNdarrayType(float32, 3D)> ''   \n | | | | | | |GpuFromHost [id BT] <CudaNdarrayType(float32, 3D)> ''   \n | | | | | |   |s1 [id T] <TensorType(float32, 3D)>\n | | | | | |MakeVector{dtype='int64'} [id BU] <TensorType(int64, vector)> ''   \n | | | | |   |Elemwise{mul,no_inplace} [id BV] <TensorType(int64, scalar)> ''   \n | | | | |   | |Shape_i{1} [id S] <TensorType(int64, scalar)> ''   \n | | | | |   | |Shape_i{0} [id BW] <TensorType(int64, scalar)> ''   \n | | | | |   |   |s1 [id T] <TensorType(float32, 3D)>\n | | | | |   |Shape_i{2} [id BX] <TensorType(int64, scalar)> ''   \n | | | | |     |s1 [id T] <TensorType(float32, 3D)>\n | | | | |GpuReshape{2} [id BY] <CudaNdarrayType(float32, matrix)> ''   \n | | | |   |GpuJoin [id BZ] <CudaNdarrayType(float32, matrix)> ''   \n | | | |   | |TensorConstant{1} [id CA] <TensorType(int8, scalar)>\n | | | |   | |W_in_to_ingate [id CB] <CudaNdarrayType(float32, matrix)>\n | | | |   | |W_in_to_forgetgate [id CC] <CudaNdarrayType(float32, matrix)>\n | | | |   | |W_in_to_cell [id CD] <CudaNdarrayType(float32, matrix)>\n | | | |   | |W_in_to_outgate [id CE] <CudaNdarrayType(float32, matrix)>\n | | | |   |MakeVector{dtype='int64'} [id CF] <TensorType(int64, vector)> ''   \n | | | |     |Shape_i{0} [id CG] <TensorType(int64, scalar)> ''   \n | | | |     | |W_in_to_ingate [id CB] <CudaNdarrayType(float32, matrix)>\n | | | |     |Elemwise{add,no_inplace} [id CH] <TensorType(int64, scalar)> ''   \n | | | |       |Shape_i{1} [id CI] <TensorType(int64, scalar)> ''   \n | | | |       | |W_in_to_outgate [id CE] <CudaNdarrayType(float32, matrix)>\n | | | |       |Shape_i{1} [id CJ] <TensorType(int64, scalar)> ''   \n | | | |       | |W_in_to_cell [id CD] <CudaNdarrayType(float32, matrix)>\n | | | |       |Shape_i{1} [id CK] <TensorType(int64, scalar)> ''   \n | | | |       | |W_in_to_ingate [id CB] <CudaNdarrayType(float32, matrix)>\n | | | |       |Shape_i{1} [id CL] <TensorType(int64, scalar)> ''   \n | | | |         |W_in_to_forgetgate [id CC] <CudaNdarrayType(float32, matrix)>\n | | | |MakeVector{dtype='int64'} [id CM] <TensorType(int64, vector)> ''   \n | | |   |Shape_i{1} [id S] <TensorType(int64, scalar)> ''   \n | | |   |Shape_i{0} [id BW] <TensorType(int64, scalar)> ''   \n | | |   |Elemwise{add,no_inplace} [id CH] <TensorType(int64, scalar)> ''   \n | | |GpuDimShuffle{x,x,0} [id CN] <CudaNdarrayType(float32, (True, True, False))> ''   \n | |   |GpuJoin [id CO] <CudaNdarrayType(float32, vector)> ''   \n | |     |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |     |b_ingate [id CP] <CudaNdarrayType(float32, vector)>\n | |     |b_forgetgate [id CQ] <CudaNdarrayType(float32, vector)>\n | |     |b_cell [id CR] <CudaNdarrayType(float32, vector)>\n | |     |b_outgate [id CS] <CudaNdarrayType(float32, vector)>\n | |ScalarFromTensor [id CT] <int64> ''   \n | | |Elemwise{switch,no_inplace} [id CU] <TensorType(int64, scalar)> ''   \n | |   |Elemwise{le,no_inplace} [id CV] <TensorType(int8, scalar)> ''   \n | |   | |Elemwise{Composite{Switch(i0, Switch(LT((i1 + i2), i3), i3, (i1 + i2)), Switch(LT(i1, i2), i1, i2))}} [id CW] <TensorType(int64, scalar)> ''   \n | |   | | |Elemwise{lt,no_inplace} [id Q] <TensorType(int8, scalar)> ''   \n | |   | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | |   | | |Shape_i{1} [id S] <TensorType(int64, scalar)> ''   \n | |   | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   |TensorConstant{0} [id BF] <TensorType(int64, scalar)>\n | |ScalarFromTensor [id CX] <int64> ''   \n | | |Elemwise{Composite{Switch(i0, i1, minimum(i2, i3))}} [id CY] <TensorType(int64, scalar)> ''   \n | |   |Elemwise{le,no_inplace} [id CV] <TensorType(int8, scalar)> ''   \n | |   |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   |Elemwise{Composite{Switch(i0, Switch(LT((i1 + i2), i3), i3, (i1 + i2)), Switch(LT(i1, i2), i1, i2))}} [id CW] <TensorType(int64, scalar)> ''   \n | |   |Shape_i{1} [id S] <TensorType(int64, scalar)> ''   \n | |Constant{1} [id CZ] <int8>\n |Subtensor{int64:int64:int8} [id DA] <TensorType(int8, (False, False, True))> ''   \n | |InplaceDimShuffle{1,0,x} [id DB] <TensorType(int8, (False, False, True))> ''   \n | | |m1 [id V] <TensorType(int8, matrix)>\n | |ScalarFromTensor [id DC] <int64> ''   \n | | |Elemwise{switch,no_inplace} [id DD] <TensorType(int64, scalar)> ''   \n | |   |Elemwise{le,no_inplace} [id DE] <TensorType(int8, scalar)> ''   \n | |   | |Elemwise{Composite{Switch(i0, Switch(LT((i1 + i2), i3), i3, (i1 + i2)), Switch(LT(i1, i2), i1, i2))}} [id DF] <TensorType(int64, scalar)> ''   \n | |   | | |Elemwise{lt,no_inplace} [id Q] <TensorType(int8, scalar)> ''   \n | |   | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | |   | | |Shape_i{1} [id U] <TensorType(int64, scalar)> ''   \n | |   | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   |TensorConstant{0} [id BF] <TensorType(int64, scalar)>\n | |ScalarFromTensor [id DG] <int64> ''   \n | | |Elemwise{Composite{Switch(i0, i1, minimum(i2, i3))}} [id DH] <TensorType(int64, scalar)> ''   \n | |   |Elemwise{le,no_inplace} [id DE] <TensorType(int8, scalar)> ''   \n | |   |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | |   |Elemwise{Composite{Switch(i0, Switch(LT((i1 + i2), i3), i3, (i1 + i2)), Switch(LT(i1, i2), i1, i2))}} [id DF] <TensorType(int64, scalar)> ''   \n | |   |Shape_i{1} [id U] <TensorType(int64, scalar)> ''   \n | |Constant{1} [id CZ] <int8>\n |GpuIncSubtensor{InplaceSet;:int64:} [id DI] <CudaNdarrayType(float32, 3D)> ''   \n | |GpuAllocEmpty [id DJ] <CudaNdarrayType(float32, 3D)> ''   \n | | |Elemwise{add,no_inplace} [id DK] <TensorType(int64, scalar)> ''   \n | | | |Elemwise{Composite{Switch(LT(maximum(i0, i1), i2), (maximum(i0, i1) + i3), (maximum(i0, i1) - i3))}} [id DL] <TensorType(int64, scalar)> ''   \n | | | | |Elemwise{Composite{((i0 - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}(i1, Composite{(((i0 - i1) // i1) + i1)}(i2, i3), i4, i3), i4, i1, i3), i4), i5), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}(i1, Composite{(((i0 - i1) // i1) + i1)}(i2, i3), i4, i3), i4, i1, i3), i4), i5)) + i3)}} [id DM] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{maximum,no_inplace} [id B] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{add,no_inplace} [id E] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{Composite{(i0 - Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i1, i2, i3, i4), i5, i6), i7), i7, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i1, i2, i3, i4), i5, i6)))}} [id DN] <TensorType(int64, scalar)> ''   \n | | | | | | |Elemwise{Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i2), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}} [id F] <TensorType(int64, scalar)> ''   \n | | | | | | |Elemwise{lt,no_inplace} [id DO] <TensorType(int8, scalar)> ''   \n | | | | | | | |Elemwise{Composite{Switch(i0, i1, Switch(AND(LT((i2 + i3), i1), GT(i4, i1)), (i5 - i6), minimum((i2 + i3), i7)))}} [id DP] <TensorType(int64, scalar)> ''   \n | | | | | | | | |Elemwise{le,no_inplace} [id I] <TensorType(int8, scalar)> ''   \n | | | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | | | |TensorConstant{-1} [id BB] <TensorType(int64, scalar)>\n | | | | | | | | |Elemwise{Composite{Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{(i0 - Switch(LT(i1, i2), i2, i1))}(i0, Composite{(i0 - Switch(GE(i1, i2), i2, i1))}(i1, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, Switch(AND(LT(i2, i1), GT(i3, i1)), (i4 - i5), maximum((i4 + i6), i2)))}(i2, i3, (i4 - i5), i5, i6, i7, i8), i3, i7), i3, i9), i7), i3), i3, i1), i3), i10), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{(i0 - Switch(LT(i1, i2), i2, i1))}(i0, Composite{(i0 - Switch(GE(i1, i2), i2, i1))}(i1, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(Composite{Switch(i0, i1, Switch(AND(LT(i2, i1), GT(i3, i1)), (i4 - i5), maximum((i4 + i6), i2)))}(i2, i3, (i4 - i5), i5, i6, i7, i8), i3, i7), i3, i9), i7), i3), i3, i1), i3), i10)}} [id BE] <TensorType(int64, scalar)> ''   \n | | | | | | | | |Elemwise{sub,no_inplace} [id J] <TensorType(int64, scalar)> ''   \n | | | | | | | | |TensorConstant{-2} [id DQ] <TensorType(int64, scalar)>\n | | | | | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | | | | |Elemwise{switch,no_inplace} [id Y] <TensorType(int64, scalar)> ''   \n | | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | |Elemwise{Composite{Switch(i0, i1, Switch(AND(LT((i2 + i3), i1), GT(i4, i1)), (i5 - i6), minimum((i2 + i3), i7)))}} [id DP] <TensorType(int64, scalar)> ''   \n | | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | | |TensorConstant{-1} [id BD] <TensorType(int8, scalar)>\n | | | | | | |Elemwise{add,no_inplace} [id BG] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | |Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}} [id D] <TensorType(int64, scalar)> ''   \n | | | | |TensorConstant{2} [id BM] <TensorType(int64, scalar)>\n | | | | |TensorConstant{1} [id CA] <TensorType(int8, scalar)>\n | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | |Shape_i{0} [id BW] <TensorType(int64, scalar)> ''   \n | | |Shape_i{1} [id DR] <TensorType(int64, scalar)> ''   \n | |   |cell_init [id DS] <CudaNdarrayType(float32, matrix)>\n | |Rebroadcast{0} [id DT] <CudaNdarrayType(float32, 3D)> ''   \n | | |GpuDimShuffle{x,0,1} [id DU] <CudaNdarrayType(float32, (True, False, False))> ''   \n | |   |GpuDot22 [id DV] <CudaNdarrayType(float32, matrix)> ''   \n | |     |GpuAlloc [id DW] <CudaNdarrayType(float32, col)> ''   \n | |     | |CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host} [id DX] <CudaNdarrayType(float32, scalar)>\n | |     | |Shape_i{0} [id BW] <TensorType(int64, scalar)> ''   \n | |     | |TensorConstant{1} [id CA] <TensorType(int8, scalar)>\n | |     |cell_init [id DS] <CudaNdarrayType(float32, matrix)>\n | |Constant{1} [id DY] <int64>\n |GpuIncSubtensor{InplaceSet;:int64:} [id DZ] <CudaNdarrayType(float32, 3D)> ''   \n | |GpuAllocEmpty [id EA] <CudaNdarrayType(float32, 3D)> ''   \n | | |Elemwise{add,no_inplace} [id EB] <TensorType(int64, scalar)> ''   \n | | | |Elemwise{Composite{Switch(LT(i0, i1), (i0 + i2), (i0 - i2))}} [id EC] <TensorType(int64, scalar)> ''   \n | | | | |Elemwise{Composite{maximum(maximum(((i0 - Switch(i1, (i2 + i3 + i4), i2)) + i4), i5), maximum(((i0 - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), Composite{((((i0 - Switch(GE(i1, i2), i2, i1)) - i3) // i3) + i3)}(Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i11, i12, i3, i4), i8, i9), i10, i4), i8, i4), i8, (Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i4), i8), Composite{Switch(LT(i0, i1), i1, i0)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i8)), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), Composite{((((i0 - Switch(GE(i1, i2), i2, i1)) - i3) // i3) + i3)}(Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i11, i12, i3, i4), i8, i9), i10, i4), i8, i4), i8, (Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i4), i8), Composite{Switch(LT(i0, i1), i1, i0)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i8))) + i4), i5))}} [id ED] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{maximum,no_inplace} [id B] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{lt,no_inplace} [id BI] <TensorType(int8, scalar)> ''   \n | | | | | |Elemwise{Composite{Switch(LT((i0 - i1), i2), (i3 + (-i4)), Switch(GE((i0 - i1), i5), (i6 + i0), Switch(LE(i5, i2), (i6 + i0), i0)))}} [id BJ] <TensorType(int64, scalar)> ''   \n | | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | |TensorConstant{2} [id BM] <TensorType(int64, scalar)>\n | | | | | |Elemwise{lt,no_inplace} [id G] <TensorType(int8, scalar)> ''   \n | | | | | |Elemwise{Composite{Switch(i0, i1, maximum(minimum((i2 + i3), i4), i5))}} [id H] <TensorType(int64, scalar)> ''   \n | | | | | |TensorConstant{0} [id W] <TensorType(int8, scalar)>\n | | | | | |TensorConstant{-1} [id BD] <TensorType(int8, scalar)>\n | | | | | |Elemwise{add,no_inplace} [id EE] <TensorType(int64, scalar)> ''   \n | | | | | | |Elemwise{minimum,no_inplace} [id R] <TensorType(int64, scalar)> ''   \n | | | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | | | |Elemwise{lt,no_inplace} [id DO] <TensorType(int8, scalar)> ''   \n | | | | | |Elemwise{Composite{Switch(i0, i1, Switch(AND(LT((i2 + i3), i1), GT(i4, i1)), (i5 - i6), minimum((i2 + i3), i7)))}} [id DP] <TensorType(int64, scalar)> ''   \n | | | | |TensorConstant{1} [id CA] <TensorType(int8, scalar)>\n | | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | | |TensorConstant{1} [id M] <TensorType(int64, scalar)>\n | | |Shape_i{0} [id BW] <TensorType(int64, scalar)> ''   \n | | |Shape_i{1} [id EF] <TensorType(int64, scalar)> ''   \n | |   |hid_init [id EG] <CudaNdarrayType(float32, matrix)>\n | |Rebroadcast{0} [id EH] <CudaNdarrayType(float32, 3D)> ''   \n | | |GpuDimShuffle{x,0,1} [id EI] <CudaNdarrayType(float32, (True, False, False))> ''   \n | |   |GpuDot22 [id EJ] <CudaNdarrayType(float32, matrix)> ''   \n | |     |GpuAlloc [id DW] <CudaNdarrayType(float32, col)> ''   \n | |     |hid_init [id EG] <CudaNdarrayType(float32, matrix)>\n | |Constant{1} [id DY] <int64>\n |GpuJoin [id EK] <CudaNdarrayType(float32, matrix)> ''   \n | |TensorConstant{1} [id CA] <TensorType(int8, scalar)>\n | |W_hid_to_ingate [id EL] <CudaNdarrayType(float32, matrix)>\n | |W_hid_to_forgetgate [id EM] <CudaNdarrayType(float32, matrix)>\n | |W_hid_to_cell [id EN] <CudaNdarrayType(float32, matrix)>\n | |W_hid_to_outgate [id EO] <CudaNdarrayType(float32, matrix)>\n |GpuDimShuffle{x,0} [id EP] <CudaNdarrayType(float32, row)> ''   \n | |W_cell_to_ingate [id EQ] <CudaNdarrayType(float32, vector)>\n |GpuDimShuffle{x,0} [id ER] <CudaNdarrayType(float32, row)> ''   \n | |W_cell_to_forgetgate [id ES] <CudaNdarrayType(float32, vector)>\n |GpuDimShuffle{x,0} [id ET] <CudaNdarrayType(float32, row)> ''   \n   |W_cell_to_outgate [id EU] <CudaNdarrayType(float32, vector)>\nforall_inplace,gpu,scan_fn}.1 [id A] <CudaNdarrayType(float32, 3D)> ''   \n\nInner graphs of the scan ops:\n\nforall_inplace,gpu,scan_fn}.0 [id A] <CudaNdarrayType(float32, 3D)> ''   \n >GpuElemwise{Switch,no_inplace} [id EV] <CudaNdarrayType(float32, matrix)> ''   \n > |GpuFromHost [id EW] <CudaNdarrayType(float32, col)> ''   \n > | |Elemwise{Cast{float32}} [id EX] <TensorType(float32, col)> ''   \n > |   |<TensorType(int8, col)> [id EY] <TensorType(int8, col)> -> [id DA]\n > |GpuElemwise{Composite{((scalar_sigmoid((i0 + (i1 * i2))) * i1) + (scalar_sigmoid((i3 + (i1 * i4))) * tanh(i5)))},no_inplace} [id EZ] <CudaNdarrayType(float32, matrix)> ''   \n > | |GpuSubtensor{::, int64:int64:} [id FA] <CudaNdarrayType(float32, matrix)> ''   \n > | | |GpuGemm{no_inplace} [id FB] <CudaNdarrayType(float32, matrix)> ''   \n > | | | |<CudaNdarrayType(float32, matrix)> [id FC] <CudaNdarrayType(float32, matrix)> -> [id BN]\n > | | | |TensorConstant{1.0} [id FD] <TensorType(float32, scalar)>\n > | | | |<CudaNdarrayType(float32, matrix)> [id FE] <CudaNdarrayType(float32, matrix)> -> [id DZ]\n > | | | |<CudaNdarrayType(float32, matrix)> [id FF] <CudaNdarrayType(float32, matrix)> -> [id EK]\n > | | | |TensorConstant{1.0} [id FD] <TensorType(float32, scalar)>\n > | | |Constant{512} [id FG] <int64>\n > | | |Constant{1024} [id FH] <int64>\n > | |<CudaNdarrayType(float32, matrix)> [id FI] <CudaNdarrayType(float32, matrix)> -> [id DI]\n > | |<CudaNdarrayType(float32, row)> [id FJ] <CudaNdarrayType(float32, row)> -> [id ER]\n > | |GpuSubtensor{::, int64:int64:} [id FK] <CudaNdarrayType(float32, matrix)> ''   \n > | | |GpuGemm{no_inplace} [id FB] <CudaNdarrayType(float32, matrix)> ''   \n > | | |Constant{0} [id FL] <int64>\n > | | |Constant{512} [id FG] <int64>\n > | |<CudaNdarrayType(float32, row)> [id FM] <CudaNdarrayType(float32, row)> -> [id EP]\n > | |GpuSubtensor{::, int64:int64:} [id FN] <CudaNdarrayType(float32, matrix)> ''   \n > |   |GpuGemm{no_inplace} [id FB] <CudaNdarrayType(float32, matrix)> ''   \n > |   |Constant{1024} [id FH] <int64>\n > |   |Constant{1536} [id FO] <int64>\n > |<CudaNdarrayType(float32, matrix)> [id FI] <CudaNdarrayType(float32, matrix)> -> [id DI]\n >GpuElemwise{Composite{Switch(i0, (scalar_sigmoid((i1 + (i2 * i3))) * tanh(i2)), i4)},no_inplace} [id FP] <CudaNdarrayType(float32, matrix)> ''   \n > |GpuFromHost [id EW] <CudaNdarrayType(float32, col)> ''   \n > |GpuSubtensor{::, int64:int64:} [id FQ] <CudaNdarrayType(float32, matrix)> ''   \n > | |GpuGemm{no_inplace} [id FB] <CudaNdarrayType(float32, matrix)> ''   \n > | |Constant{1536} [id FO] <int64>\n > | |Constant{2048} [id FR] <int64>\n > |GpuElemwise{Composite{((scalar_sigmoid((i0 + (i1 * i2))) * i1) + (scalar_sigmoid((i3 + (i1 * i4))) * tanh(i5)))},no_inplace} [id EZ] <CudaNdarrayType(float32, matrix)> ''   \n > |<CudaNdarrayType(float32, row)> [id FS] <CudaNdarrayType(float32, row)> -> [id ET]\n > |<CudaNdarrayType(float32, matrix)> [id FE] <CudaNdarrayType(float32, matrix)> -> [id DZ]\n\nforall_inplace,gpu,scan_fn}.1 [id A] <CudaNdarrayType(float32, 3D)> ''   \n >GpuElemwise{Switch,no_inplace} [id EV] <CudaNdarrayType(float32, matrix)> ''   \n >GpuElemwise{Composite{Switch(i0, (scalar_sigmoid((i1 + (i2 * i3))) * tanh(i2)), i4)},no_inplace} [id FP] <CudaNdarrayType(float32, matrix)> ''   \n\nStorage map footprint:\n - GpuElemwise{add,no_inplace}.0, Shape: (212, 6, 2048), ElemSize: 4 Byte(s), TotalSize: 10420224 Byte(s)\n - GpuSubtensor{int64:int64:int8}.0, Shape: (212, 6, 2048), ElemSize: 4 Byte(s), TotalSize: 10420224 Byte(s)\n - GpuJoin.0, Shape: (512, 2048), ElemSize: 4 Byte(s), TotalSize: 4194304 Byte(s)\n - forall_inplace,gpu,scan_fn}.1, Shape: (213, 6, 512), ElemSize: 4 Byte(s), TotalSize: 2617344 Byte(s)\n - GpuIncSubtensor{InplaceSet;:int64:}.0, Shape: (213, 6, 512), ElemSize: 4 Byte(s), TotalSize: 2617344 Byte(s)\n - GpuIncSubtensor{InplaceSet;:int64:}.0, Shape: (213, 6, 512), ElemSize: 4 Byte(s), TotalSize: 2617344 Byte(s)\n - forall_inplace,gpu,scan_fn}.0, Shape: (213, 6, 512), ElemSize: 4 Byte(s), TotalSize: 2617344 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (2550, 128), ElemSize: 4 Byte(s), TotalSize: 1305600 Byte(s)\n - W, Shared Input, Shape: (2550, 128), ElemSize: 4 Byte(s), TotalSize: 1305600 Byte(s)\n - W_hid_to_ingate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_forgetgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_cell, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_ingate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_outgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_forgetgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_outgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_forgetgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_forgetgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_cell, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_outgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_ingate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_outgate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_cell, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_ingate, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - W_hid_to_cell, Shared Input, Shape: (512, 512), ElemSize: 4 Byte(s), TotalSize: 1048576 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - GpuReshape{2}.0, Shape: (1272, 30), ElemSize: 4 Byte(s), TotalSize: 152640 Byte(s)\n - s1, Input, Shape: (6, 212, 30), ElemSize: 4 Byte(s), TotalSize: 152640 Byte(s)\n - s2, Input, Shape: (6, 147, 30), ElemSize: 4 Byte(s), TotalSize: 105840 Byte(s)\n - W_in_to_cell, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_forgetgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_cell, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_ingate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_ingate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_forgetgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_outgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_forgetgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_ingate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_cell, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_outgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_ingate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_forgetgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_outgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_cell, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - W_in_to_outgate, Shared Input, Shape: (30, 512), ElemSize: 4 Byte(s), TotalSize: 61440 Byte(s)\n - <CudaNdarrayType(float32, 4D)>, Shared Input, Shape: (10, 20, 3, 3), ElemSize: 4 Byte(s), TotalSize: 7200 Byte(s)\n - W, Shared Input, Shape: (10, 20, 3, 3), ElemSize: 4 Byte(s), TotalSize: 7200 Byte(s)\n - cell_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - GpuDimShuffle{x,0}.0, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_cell, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - hid_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - GpuDimShuffle{x,0}.0, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - GpuDimShuffle{x,0}.0, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_cell, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - cell_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - hid_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_cell, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - cell_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - hid_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_cell, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b_outgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - cell_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - hid_init, Shared Input, Shape: (1, 512), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_ingate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - W_cell_to_forgetgate, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (128, 3), ElemSize: 4 Byte(s), TotalSize: 1536 Byte(s)\n - W, Shared Input, Shape: (128, 3), ElemSize: 4 Byte(s), TotalSize: 1536 Byte(s)\n - InplaceDimShuffle{1,0,x}.0, Shape: (212, 6, 1), ElemSize: 1 Byte(s), TotalSize: 1272 Byte(s)\n - m1, Input, Shape: (6, 212), ElemSize: 1 Byte(s), TotalSize: 1272 Byte(s)\n - Subtensor{int64:int64:int8}.0, Shape: (212, 6, 1), ElemSize: 1 Byte(s), TotalSize: 1272 Byte(s)\n - m2, Input, Shape: (6, 147), ElemSize: 1 Byte(s), TotalSize: 882 Byte(s)\n - <CudaNdarrayType(float32, 4D)>, Shared Input, Shape: (20, 1, 3, 3), ElemSize: 4 Byte(s), TotalSize: 720 Byte(s)\n - W, Shared Input, Shape: (20, 1, 3, 3), ElemSize: 4 Byte(s), TotalSize: 720 Byte(s)\n - b, Shared Input, Shape: (128,), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (128,), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (20,), ElemSize: 4 Byte(s), TotalSize: 80 Byte(s)\n - b, Shared Input, Shape: (20,), ElemSize: 4 Byte(s), TotalSize: 80 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (10,), ElemSize: 4 Byte(s), TotalSize: 40 Byte(s)\n - b, Shared Input, Shape: (10,), ElemSize: 4 Byte(s), TotalSize: 40 Byte(s)\n - TensorConstant{[ -1   1   4 512]}, Shape: (4,), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - GpuAlloc.0, Shape: (6, 1), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)\n - ent, Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)\n - TensorConstant{(2,) of 4}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{(2,) of 2}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{(2,) of 0}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - b, Shared Input, Shape: (3,), ElemSize: 4 Byte(s), TotalSize: 12 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (3,), ElemSize: 4 Byte(s), TotalSize: 12 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{4}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{512}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{20}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{minimum(maximum(maximum((i0 - i1), (i0 - i1)), ((i2 + i1) - i1)), i3)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(i0, Switch(LT((i1 + i2), i3), i3, (i1 + i2)), Switch(LT(i1, i2), i1, i2))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{maximum,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(i0, Switch(LT((i1 + i2), i3), i3, (i1 + i2)), Switch(LT(i1, i2), i1, i2))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{add,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{-2}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{((i0 - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}(i1, Composite{(((i0 - i1) // i1) + i1)}(i2, i3), i4, i3), i4, i1, i3), i4), i5), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}(i1, Composite{(((i0 - i1) // i1) + i1)}(i2, i3), i4, i3), i4, i1, i3), i4), i5)) + i3)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{2550}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{-1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{sub,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{2}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(LT(i0, i1), (i0 + i2), (i0 - i2))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{2048}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(LT(i0, i1), i1, i0)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{add,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(i0, (i1 + i2 + i3), i1)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{10}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{add,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(LT(maximum(i0, i1), i2), (maximum(i0, i1) + i3), (maximum(i0, i1) - i3))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i2), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{2}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{minimum,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{add,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{(i0 - Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i1, i2, i3, i4), i5, i6), i7), i7, Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i1, i2, i3, i4), i5, i6)))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{add,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{mul,no_inplace}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{-1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{maximum(maximum(((i0 - Switch(i1, (i2 + i3 + i4), i2)) + i4), i5), maximum(((i0 - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), Composite{((((i0 - Switch(GE(i1, i2), i2, i1)) - i3) // i3) + i3)}(Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i11, i12, i3, i4), i8, i9), i10, i4), i8, i4), i8, (Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i4), i8), Composite{Switch(LT(i0, i1), i1, i0)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i8)), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i2 - i3), i0)}(Composite{((i0 - (Switch(LT(i1, i2), i2, i1) - i3)) - i3)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), Composite{((((i0 - Switch(GE(i1, i2), i2, i1)) - i3) // i3) + i3)}(Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i11, i12, i3, i4), i8, i9), i10, i4), i8, i4), i8, (Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i4), i8), Composite{Switch(LT(i0, i1), i1, i0)}((Composite{Switch(GE(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5), i6), (i6 - i3), Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(i0, (i1 + i2 + i3), i1)}(i0, i1, i2, i3), i4, i5))}(i6, i7, i3, i4, i8, i9, i10) + i4), i8))) + i4), i5))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{255}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{0.990000009537}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - TensorConstant{0.00999999977648}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{1.00999999046}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - Constant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Constant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - TensorConstant{1.99999999495e-06}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{error while transferring the value: error (an illegal instruction was encountered)copying data to host}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - Constant{0.01}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Constant{1}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{2}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{-1}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Elemwise{lt,no_inplace}.0, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n TotalSize: 59283384.0 Byte(s) 0.055 GB\n TotalSize inputs: 39281239.0 Byte(s) 0.037 GB\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'."
     ]
    }
   ],
   "source": [
    "# train network (assuming you've got some training data in numpy arrays)\n",
    "\n",
    "def makeMask(batch): #list of matricies of variable size.\n",
    "    m = np.zeros((len(batch), max([b.shape[0] for b in batch])))\n",
    "    for i,b in enumerate(batch):\n",
    "        m[i, 0:b.shape[0]] = 1.0\n",
    "    return m;\n",
    "\n",
    "for epoch in range(20):\n",
    "    batchCount = 0\n",
    "    loss = 0\n",
    "    print \"HOLY SHIT IT EPOCHS!\"\n",
    "    #for (batch1,m1), (batch2,m2), ys in dataIO.readChunk(kBATCH_SIZE, './snli_1.0/snli_1.0_train.txt'):\n",
    "    for (batch1,m1), (batch2,m2), ys in dataIO.readChunk(kBATCH_SIZE, './snli_1.0/snli_1.0_train.txt'):\n",
    "        #         if batchCount >= kMAX_BATCHES:\n",
    "        #             break\n",
    "        # import code\n",
    "        # code.interact(local=locals())\n",
    "        # print batch1.shape, batch2.shape, ys.shape\n",
    "        # loss += train_fn(batch1, batch2, makeMask(batch1), makeMask(batch2), ys)\n",
    "        loss += train_fn(batch1, batch2, m1, m2, ys)\n",
    "        batchCount += 1\n",
    "        if batchCount > 5000:\n",
    "            break\n",
    "\n",
    "    print(\"Epoch %d: Loss %g\" % (epoch + 1, loss / batchCount))\n",
    "   \n",
    "    vals = lasagne.layers.get_all_param_values(network)\n",
    "    modelFile = open(\"modelStore/\"+time.strftime(\"%m%d-%H%M%S\")+\"-E\"+str(epoch)+\".pkl\", mode=\"w\")    \n",
    "    np.savez(modelFile, model=vals)\n",
    "    print \">>>\", modelFile.name\n",
    "    modelFile.close()\n",
    "    print \"Done.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> modelStore/0602-165522.pkl\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "vals = lasagne.layers.get_all_param_values(network)\n",
    "modelFile = open(\"modelStore/\"+time.strftime(\"%m%d-%H%M%S\")+\".pkl\", mode=\"w\")    \n",
    "np.savez(modelFile, model=vals)\n",
    "print \">>>\", modelFile.name\n",
    "modelFile.close()\n",
    "print \"Done.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use trained network for predictions\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "predict_fn = theano.function([sentence1, sentence2, mask1, mask2], T.argmax(test_prediction, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-10ef9fa02c5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtotal_incorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataIO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadChunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./snli_1.0/snli_1.0_dev.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "# Check on dev set\n",
    "batch_num = 0\n",
    "total_classifications = 0\n",
    "total_correct = 0\n",
    "total_incorrect = 0\n",
    "for (batch1,m1), (batch2,m2), ys in dataIO.readChunk(kBATCH_SIZE, './snli_1.0/snli_1.0_dev.txt'):\n",
    "    results = predict_fn(batch1, batch2, m1, m2);\n",
    "    for i in range(len(results)):\n",
    "        if results[i] == ys[i]:\n",
    "            total_correct += 1\n",
    "        else:\n",
    "            total_incorrect += 1\n",
    "        total_classifications += 1\n",
    "    batch_num += 1\n",
    "    print batch_num, \"\\r\", \n",
    "    #if batch_num == 1666: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classifications: 2394\n",
      "Total correct: 847\n",
      "Total incorrect: 1547\n"
     ]
    }
   ],
   "source": [
    "# Stats print out\n",
    "\n",
    "print \"Total classifications: \" + str(total_classifications)\n",
    "print \"Total correct: \" + str(total_correct)\n",
    "print \"Total incorrect: \" + str(total_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "# Check on test set\n",
    "batch_num = 0\n",
    "total_classifications = 0\n",
    "total_correct = 0\n",
    "total_incorrect = 0\n",
    "for (batch1,m1), (batch2,m2), ys in dataIO.readChunk(kBATCH_SIZE, './snli_1.0/snli_1.0_train.txt'):\n",
    "    if batch_num == 500:\n",
    "        break\n",
    "    results = predict_fn(batch1, batch2, m1, m2)\n",
    "    for i in range(len(results)):\n",
    "        if results[i] == ys[i]:\n",
    "            total_correct += 1\n",
    "        else:\n",
    "            total_incorrect += 1\n",
    "        total_classifications += 1\n",
    "    batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classifications: 600\n",
      "Total correct: 199\n",
      "Total incorrect: 401\n"
     ]
    }
   ],
   "source": [
    "# Stats print out for train set\n",
    "\n",
    "print \"Total classifications: \" + str(total_classifications)\n",
    "print \"Total correct: \" + str(total_correct)\n",
    "print \"Total incorrect: \" + str(total_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
