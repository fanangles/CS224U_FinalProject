{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): ERROR: Not using GPU. Initialisation of device 1 failed:\n",
      "initCnmem: cnmemInit call failed! Reason=CNMEM_STATUS_OUT_OF_MEMORY. numdev=1\n",
      "\n",
      "ERROR:theano.sandbox.cuda:ERROR: Not using GPU. Initialisation of device 1 failed:\n",
      "initCnmem: cnmemInit call failed! Reason=CNMEM_STATUS_OUT_OF_MEMORY. numdev=1\n",
      "\n",
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled with initial size: 90.0% of memory, cuDNN 4007)\n",
      "/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np;\n",
    "import time\n",
    "\n",
    "from const import *;\n",
    "\n",
    "# create Theano variables for input and target minibatch\n",
    "sentence1 = T.ftensor3('s1')\n",
    "sentence2 = T.ftensor3('s2')\n",
    "mask1 = T.bmatrix('m1')\n",
    "mask2 = T.bmatrix('m2')\n",
    "target_var = T.ivector('ent')\n",
    "MAX_BATCHES = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a small convolutional neural network\n",
    "from lasagne.nonlinearities import leaky_rectify, softmax\n",
    "from lasagne.layers import InputLayer, Conv2DLayer, Pool2DLayer, LSTMLayer, ConcatLayer, DenseLayer, dropout, Conv1DLayer, ReshapeLayer\n",
    "\n",
    "\n",
    "\n",
    "def createNeuralNetwork():\n",
    "    #(batchsize, sequence length, onehot vector length)\n",
    "    in1 = InputLayer((None, None, kNUM_CHARS), sentence1)\n",
    "    in2 = InputLayer((None, None, kNUM_CHARS), sentence2)\n",
    "    l_mask1=lasagne.layers.InputLayer((None,None), mask1)\n",
    "    l_mask2=lasagne.layers.InputLayer((None,None), mask2)\n",
    "\n",
    "    num_LSTM_output = (512)\n",
    "    lstm1_f = LSTMLayer(in1, num_LSTM_output,\n",
    "        forgetgate=lasagne.layers.Gate(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        cell_init=lasagne.init.Constant(0.),\n",
    "        hid_init=lasagne.init.Constant(0.), grad_clipping=kGRAD_CLIP,\n",
    "        backwards=False,\n",
    "        mask_input=l_mask1,\n",
    "        only_return_final=True)\n",
    "    lstm1_b = LSTMLayer(in1, num_LSTM_output,\n",
    "        forgetgate=lasagne.layers.Gate(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        cell_init=lasagne.init.Constant(0.),\n",
    "        hid_init=lasagne.init.Constant(0.), grad_clipping=kGRAD_CLIP,\n",
    "        backwards=True,\n",
    "        mask_input=l_mask1,\n",
    "        only_return_final=True)\n",
    "\n",
    "    lstm2_f = LSTMLayer(in2, num_LSTM_output,\n",
    "        forgetgate=lasagne.layers.Gate(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        cell_init=lasagne.init.Constant(0.),\n",
    "        hid_init=lasagne.init.Constant(0.), grad_clipping=kGRAD_CLIP,\n",
    "        backwards=False,\n",
    "        mask_input=l_mask2,\n",
    "        only_return_final=True)\n",
    "    lstm2_b = LSTMLayer(in2, num_LSTM_output,\n",
    "        forgetgate=lasagne.layers.Gate(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        cell_init=lasagne.init.Constant(0.),\n",
    "        hid_init=lasagne.init.Constant(0.), grad_clipping=kGRAD_CLIP,\n",
    "        backwards=True,\n",
    "        mask_input=l_mask2,\n",
    "        only_return_final=True)\n",
    "\n",
    "    network = ConcatLayer([lstm1_f, lstm1_b, lstm2_f, lstm2_b], axis=1) #(NONE-sentencesize by 2048)\n",
    "    network = ReshapeLayer(network, (-1, 1, 4, num_LSTM_output));\n",
    "    #(None by 4x512) I think.\n",
    "    network = Conv2DLayer(network, 20, (3,3), pad='same',\n",
    "                                         nonlinearity=leaky_rectify)\n",
    "    #(20 by 4 by 52)\n",
    "    network = Conv2DLayer(network, 10, (3,3), pad='same',\n",
    "                                         nonlinearity=leaky_rectify)\n",
    "\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, (4,4), stride=2)\n",
    "\n",
    "    network = DenseLayer(dropout(network, 0.5),\n",
    "                                        128, nonlinearity=leaky_rectify,\n",
    "                                        W=lasagne.init.Orthogonal())\n",
    "\n",
    "    network = DenseLayer(dropout(network, 0.5), 3, nonlinearity=softmax)\n",
    "\n",
    "    return network;\n",
    "\n",
    "\n",
    "network = createNeuralNetwork();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Neural Network Values from File\n",
      "Loaded!\n",
      "HOLY SHIT IT COMPILED\n",
      "HOLY SHIT IT COMPILED OUTPUT\n",
      "HOLY SHIT IT COMPILED LOSS FUNCTION WHAT\n",
      "HOLY SHIT IT COMPILED UPDATES\n",
      "HOLY SHIT IT COMPILED TRAINING FUNCTION\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def loadDriverModelFromFile(filename):\n",
    "    print \"Loading Neural Network Values from File\"\n",
    "    _v = np.load(filename)['model']\n",
    "    lasagne.layers.set_all_param_values(network, _v)\n",
    "    if type(filename)==str:\n",
    "        print \"LOADED\"\n",
    "        return;\n",
    "    print \"Loaded!\"\n",
    "\n",
    "with open(\"./modelStore/0606-184240-E4.pkl\", 'r') as f:\n",
    "    loadDriverModelFromFile(f)\n",
    "\n",
    "print \"HOLY SHIT IT COMPILED\"\n",
    "# create loss function\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "print \"HOLY SHIT IT COMPILED OUTPUT\"\n",
    "\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean() + 1e-4 * lasagne.regularization.regularize_network_params(\n",
    "        network, lasagne.regularization.l2)\n",
    "print \"HOLY SHIT IT COMPILED LOSS FUNCTION WHAT\"\n",
    "\n",
    "# create parameter update expressions\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01,\n",
    "                                            momentum=0.9)\n",
    "print \"HOLY SHIT IT COMPILED UPDATES\"\n",
    "\n",
    "# compile training function that updates parameters and returns training loss\n",
    "train_fn = theano.function([sentence1, sentence2, mask1, mask2, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "# train_fn = theano.function([sentence1, sentence2, target_var], loss, updates=updates)\n",
    "print \"HOLY SHIT IT COMPILED TRAINING FUNCTION\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOLY SHIT IT EPOCHS!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kBATCH_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fe49ff9536d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"HOLY SHIT IT EPOCHS!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#for (batch1,m1), (batch2,m2), ys in dataIO.readChunk(kBATCH_SIZE, './snli_1.0/snli_1.0_train.txt'):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataIO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadChunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./snli_1.0/snli_1.0_train.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;31m#         if batchCount >= kMAX_BATCHES:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#             break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kBATCH_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "# train network (assuming you've got some training data in numpy arrays)\n",
    "\n",
    "def makeMask(batch): #list of matricies of variable size.\n",
    "    m = np.zeros((len(batch), max([b.shape[0] for b in batch])))\n",
    "    for i,b in enumerate(batch):\n",
    "        m[i, 0:b.shape[0]] = 1.0\n",
    "    return m;\n",
    "\n",
    "for epoch in range(10):\n",
    "    batchCount = 0\n",
    "    loss = 0\n",
    "    print \"HOLY SHIT IT EPOCHS!\"\n",
    "    #for (batch1,m1), (batch2,m2), ys in dataIO.readChunk(kBATCH_SIZE, './snli_1.0/snli_1.0_train.txt'):\n",
    "    for (batch1,m1), (batch2,m2), ys in dataIO.readChunk(kBATCH_SIZE, './snli_1.0/snli_1.0_train.txt'):\n",
    "        #         if batchCount >= kMAX_BATCHES:\n",
    "        #             break\n",
    "        # import code\n",
    "        # code.interact(local=locals())\n",
    "        # print batch1.shape, batch2.shape, ys.shape\n",
    "        # loss += train_fn(batch1, batch2, makeMask(batch1), makeMask(batch2), ys)\n",
    "        loss += train_fn(batch1, batch2, m1, m2, ys)\n",
    "        batchCount += 1\n",
    "        if batchCount > 8000:\n",
    "            break\n",
    "\n",
    "    print(\"Epoch %d: Loss %g\" % (epoch + 1, loss / batchCount))\n",
    "   \n",
    "    vals = lasagne.layers.get_all_param_values(network)\n",
    "    modelFile = open(\"modelStore/\"+time.strftime(\"%m%d-%H%M%S\")+\"-E\"+str(epoch)+\".pkl\", mode=\"w\")    \n",
    "    np.savez(modelFile, model=vals)\n",
    "    print \">>>\", modelFile.name\n",
    "    modelFile.close()\n",
    "    print \"Done.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> modelStore/0606-160303.pkl\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "vals = lasagne.layers.get_all_param_values(network)\n",
    "modelFile = open(\"modelStore/\"+time.strftime(\"%m%d-%H%M%S\")+\".pkl\", mode=\"w\")    \n",
    "np.savez(modelFile, model=vals)\n",
    "print \">>>\", modelFile.name\n",
    "modelFile.close()\n",
    "print \"Done.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use trained network for predictions\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "predict_fn = theano.function([sentence1, sentence2, mask1, mask2], T.argmax(test_prediction, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-eda8492214fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataIO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadChunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./snli_1.0/snli_1.0_dev.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/xiaonan/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "# Check on dev set\n",
    "batch_num = 0\n",
    "total_classifications = 0\n",
    "total_correct = 0\n",
    "total_incorrect = 0\n",
    "count = dict()\n",
    "for (batch1,m1), (batch2,m2), ys in dataIO.readChunk(kBATCH_SIZE, './snli_1.0/snli_1.0_dev.txt'):\n",
    "    results = predict_fn(batch1, batch2, m1, m2);\n",
    "    for i in range(len(results)):\n",
    "        if results[i] == ys[i]:\n",
    "            total_correct += 1\n",
    "        else:\n",
    "            total_incorrect += 1\n",
    "        total_classifications += 1\n",
    "        if results[i] in count:\n",
    "            count[results[i]] += 1\n",
    "        else:\n",
    "            count[results[i]] = 1\n",
    "    batch_num += 1\n",
    "    print batch_num, \"\\r\",\n",
    "print count\n",
    "    #if batch_num == 1666: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classifications: 10000\n",
      "Total correct: 3329\n",
      "Total incorrect: 6671\n"
     ]
    }
   ],
   "source": [
    "# Stats print out\n",
    "\n",
    "print \"Total classifications: \" + str(total_classifications)\n",
    "print \"Total correct: \" + str(total_correct)\n",
    "print \"Total incorrect: \" + str(total_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading \n",
      "{0: 5000}\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "# Check on test set\n",
    "batch_num = 0\n",
    "total_classifications = 0\n",
    "total_correct = 0\n",
    "total_incorrect = 0\n",
    "count = dict()\n",
    "for (batch1,m1), (batch2,m2), ys in dataIO.readChunk(kBATCH_SIZE, './snli_1.0/snli_1.0_train.txt'):\n",
    "    if batch_num == 5000:\n",
    "        break\n",
    "    results = predict_fn(batch1, batch2, m1, m2)\n",
    "    for i in range(len(results)):\n",
    "        if results[i] == ys[i]:\n",
    "            total_correct += 1\n",
    "        else:\n",
    "            total_incorrect += 1\n",
    "        total_classifications += 1\n",
    "        if results[i] in count:\n",
    "            count[results[i]] += 1\n",
    "        else:\n",
    "            count[results[i]] = 1\n",
    "    batch_num += 1\n",
    "    print batch_num, \"\\r\", \n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classifications: 5000\n",
      "Total correct: 1662\n",
      "Total incorrect: 3338\n"
     ]
    }
   ],
   "source": [
    "# Stats print out for train set\n",
    "\n",
    "print \"Total classifications: \" + str(total_classifications)\n",
    "print \"Total correct: \" + str(total_correct)\n",
    "print \"Total incorrect: \" + str(total_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
